<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>paulolivier.dehaye.org (privacy)</title><link>http://paulolivier.dehaye.org/</link><description></description><language>en</language><lastBuildDate>Fri, 26 Sep 2014 00:56:02 GMT</lastBuildDate><generator>http://getnikola.com/</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Click here so I can tell you about privacy (and invade yours too)</title><link>http://paulolivier.dehaye.org/posts/click-here-so-i-can-teach-you-about-privacy-and-invade-it-too.html</link><dc:creator>Paul-Olivier Dehaye</dc:creator><description>&lt;p&gt;Simply by looking at this page, you agree to this site's privacy policy, which is a copy of the Swiss Railway's statement on the use of Google Analytics:&lt;/p&gt;
&lt;blockquote class="epigraph"&gt;
&lt;p&gt;“This website uses Google Analytics, a web analytics service provided by Google, Inc. (“Google”). Google Analytics uses “cookies”, which are text files placed on your computer, to help the website analyze how users use the site. The information generated by the cookie about your use of the website (including your IP address) will be transmitted to and stored by Google on servers in the United States . Google will use this information for the purpose of evaluating your use of the website, compiling reports on website activity for website operators and providing other services relating to website activity and internet usage. Google may also transfer this information to third parties where required to do so by law, or where such third parties process the information on Google's behalf. Google will not associate your IP address with any other data held by Google. You may refuse the use of cookies by selecting the appropriate settings on your browser, however please note that if you do this you may not be able to use the full functionality of this website. By using this website, you consent to the processing of data about you by Google in the manner and for the purposes set out above.”&lt;/p&gt;
&lt;p class="attribution"&gt;—&lt;a class="reference external" href="https://support.google.com/analytics/answer/6004245"&gt;Google Analytics Terms of Service&lt;/a&gt;, as on the &lt;a class="reference external" href="http://www.sbb.ch/en/meta/data-protection/data-protection-google-analytics-statement.html"&gt;SBB website&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The site itself does not collect data. However I do use Google Analytics, which requires me to introduce this policy as part of the service. The service allows me to track users on this website in various ways, and produces beautiful graphics like this one:&lt;/p&gt;
&lt;img alt="../google-analytics.jpg" class="align-center" src="http://paulolivier.dehaye.org/google-analytics.jpg" style="width: 1020.0px; height: 518.0px;"&gt;&lt;p&gt;The comments below are bound by the Disqus Terms of Service, which are &lt;a class="reference external" href="https://help.disqus.com/customer/portal/articles/466259-privacy-policy"&gt;available here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If you don't like it, you can leave. So is the law of the internet.&lt;/p&gt;</description><category>connected_course</category><category>edtech</category><category>privacy</category><category>switzerland</category><guid>http://paulolivier.dehaye.org/posts/click-here-so-i-can-teach-you-about-privacy-and-invade-it-too.html</guid><pubDate>Wed, 10 Sep 2014 06:44:21 GMT</pubDate></item><item><title>Edtech policies (part I)</title><link>http://paulolivier.dehaye.org/posts/edtech-policies-part-i.html</link><dc:creator>Paul-Olivier Dehaye</dc:creator><description>&lt;p&gt;In a previous post, I talked about some of the problems associated with &lt;a class="reference external" href="http://paulolivier.dehaye.org/posts/dont-be-evil-or-how-i-learned-to-behave-like-a-startup-and-love-the-data.html"&gt;data collection with no clear purpose&lt;/a&gt;. In this one, I want to compare two of the big players in edtech on a narrow point, that of comment posts and their associated privacy. This is partly done in response to &lt;a class="reference external" href="https://twitter.com/funnymonkey"&gt;Bill Fitzgerald&lt;/a&gt;'s posts on edtech privacy policies, but also as a concerned parent looking a bit far out in the future.&lt;/p&gt;
&lt;p&gt;On June 26th Google held an I/O developer conference. I was hoping, like many, some kind of announcement about &lt;a class="reference external" href="http://www.mooc.org"&gt;mooc.org&lt;/a&gt;. Not much was said on that, but it was still an instructive watch of the efforts of Google in the MOOC space. I was particularly struck by a comment of Julia Wilkowski (leader in their MOOC project) following a question from the audience. The comment is at 28:35, if it does not play for you:&lt;/p&gt;
&lt;embed&gt;&lt;iframe width="640" height="360" src="//www.youtube.com/embed/YCUZ01yFtsM" frameborder="0" allowfullscreen&gt;&lt;/iframe&gt;
&lt;/embed&gt;&lt;p&gt;She informs us that no sophisticated analysis has been performed thus far on the Google MOOC forum posts. There are apparently two reasons: it is complicated, and they are required to delete all the posts after 60 days to comply with their privacy policy. Indeed, Google MOOCs fall under the global umbrella of the Google Privacy Policy, which seems to uniformly apply to all their products. The main reason for this number seems to be technical rather than anything else (the backup system is presumably very complex, extending all the way to physical tapes), since I couldn't find a reference to it in the Google Privacy Policy (and neither could other people, if you, well, google it).&lt;/p&gt;
&lt;p&gt;A tad later, Peter Norvig talks about classifiers (similar to those used in the recent Facebook experiment that would make the news 3 or 4 days later), for instance to help determine when a student might be confused, a classic trick in intelligent tutoring systems. He immediately reminds us though:&lt;/p&gt;
&lt;blockquote class="epigraph"&gt;
&lt;p&gt;But there still are a lot of privacy issues involved in what [..] information can you keep, how much can you tie the identity in the forum to the identity of the student, can you tie that to their identity someplace else, and the field as a whole has to come to grips with the privacy issues so we can share and learn what we want without violating privacy.&lt;/p&gt;
&lt;p class="attribution"&gt;—Peter Norvig&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Coursera is another big player in the market, with a rather different approach. In their Terms of Service and Privacy policy, one can find the following:&lt;/p&gt;
&lt;img alt="../forum-reuse.jpg" class="align-center" src="http://paulolivier.dehaye.org/forum-reuse.jpg" style="width: 868.0px; height: 112.0px;"&gt;&lt;p&gt;The most remarkable sentence here is: &lt;strong&gt;We also reserve the right to reuse Forum posts containing Personally Identifiable Information in future versions of the course we offer, and to enhance further course offerings.&lt;/strong&gt; This sentence is very puzzling to me. What does it mean? The only logical explanation I can offer is that Coursera plans to repopulate forums in later iterations of the course with posts from a previous run, presumably one where there was more emphasis on moderating the course. Ask a dumb question once, and it will be asked again on repeat, &lt;strong&gt;in your name&lt;/strong&gt;, in contexts you don't necessarily know. Share a bit too much of your great idea for a startup, in a context where you feel confortable, well, too bad: it might be reshared again, even if you delete your post at the end of a course (many MOOC students have the feeble illusion that this protects their intellectual property).&lt;/p&gt;
&lt;p&gt;The Terms of Service also include the following, which is classically &lt;a class="reference external" href="https://www.google.ch/webhp?sourceid=chrome-instant&amp;amp;ion=1&amp;amp;espv=2&amp;amp;ie=UTF-8#q=%22Neither+the+User+Content+(as+defined+below)+on+these+Sites%2C+nor+any+links+to+other+websites%2C+are+screened%2C+moderated%2C+approved%2C+reviewed+or+endorsed%22&amp;amp;start=0"&gt;present in many platform disclaimers&lt;/a&gt;:&lt;/p&gt;
&lt;img alt="../forum-disclaimer.jpg" class="align-center" src="http://paulolivier.dehaye.org/forum-disclaimer.jpg" style="width: 863.0px; height: 156.0px;"&gt;&lt;p&gt;The Coursera Terms of Service include numerous such disturbing clauses, as detailed in &lt;a class="reference external" href="http://www.craigbutosi.ca/blog/coursera-or-socrates-was-not-a-content-provider-the-university-of-toronto-and-coursera-agreement"&gt;this wonderful post&lt;/a&gt; from August 2012. That post is a &lt;strong&gt;very highly recommended read&lt;/strong&gt;, since it details issues of free speech and academic freedom, a hot button those days. It also includes the following gem:&lt;/p&gt;
&lt;blockquote&gt;
It is crucial that we pay close attention to the fine print, something unfortunately overshadowed by the immediacy and novelty of Web 2.0 solutions and the latest trends in brand management techniques.&lt;/blockquote&gt;
&lt;p&gt;I can only recommend this approach, but it has to be moderated by a quote given earlier in the piece:&lt;/p&gt;
&lt;blockquote&gt;
Because Coursera mediates between instructor/university and user/student communication, we are dealing with at least four major relationships: user-Coursera, Coursera-instructor/university, user-Coursera-instructor/university, and vice versa. I am mainly focusing on the user-Coursera relation (terms of use and privacy policy), but it should be noted that these are really only separable at the analytical level. In reality, all of these relations are in play at any given time.&lt;/blockquote&gt;
&lt;p&gt;This leaves many unanswered questions, which are not easy to address. It requires access to other contracts, between the instructor, the university and Coursera itself.&lt;/p&gt;
&lt;p&gt;Only one such contract between a university and Coursera has been discussed on a wide scale, the contract given to the University of Michigan. It was the object of a 2012 Chronicle of Higher Education article (an antiquity in the domain of MOOCs), and is based on a Freedom of Information request for the &lt;a class="reference external" href="http://chronicle.com/article/Document-Examine-the-U-of/133063/"&gt;University of Michigan contracts&lt;/a&gt;.
More recently, the UCSC Faculty Union has entered negociations with Coursera, that are &lt;a class="reference external" href="http://ucscfa.org/2013/06/scfas-ongoing-discussion-concerning-ucscs-contract-with-coursera/"&gt;detailed on its blog&lt;/a&gt;. From the outside, these negociations seem very one-sided and highlight differences with the University of Michigan contract:&lt;/p&gt;
&lt;blockquote&gt;
In the Michigan contract, the instructor grants to COURSERA various rights FOR THE DURATION SUCH CONTENT IS OFFERED THROUGH THE PLATFORM (i.e., very limited transfer  of rights).  In our contract, in contrast, the rights are granted TO THE UNIVERSITY and this appears to be irrevocable and not connected to the hosting of the course on the Coursera  platform.&lt;/blockquote&gt;
&lt;p&gt;In other words, the balance of the Coursera contract shifts towards the instructor at the University of Michigan, compared to at UCSC. In the latest Shangai rankings (for the little they are worth), Michigan was ranked 22nd while UCSC was 93rd. Bear in mind that Michigan joined earlier, which might also affect this complex bargaining equation.&lt;/p&gt;
&lt;p&gt;A few other contracts have been put online, intentionally or not, and can be found by googling titles etc. I found eight in total, which can serve as evidence of subtle shifts of the Coursera strategy, and also segmentation according to the characteristics of the universities involved (European vs. American, public vs. private, etc). Since many of those contracts are locked under Non Disclosure Agreements, and this has already become a union issue elsewhere, I can only encourage other academics to push for openness of those contracts at their own institutions.&lt;/p&gt;
&lt;p&gt;(For a cool application of machine learning to the process of teaching, look also at the video around the 4:30 mark. Google seems to focus there on content rather than users, and to extract values from the student contributions rather than their private data. They effectively intend to crowdsource smarter compilers.)&lt;/p&gt;
&lt;p&gt;(I want to thank Ignacio Despujol Zabala for letting me know about this Google I/O session.)&lt;/p&gt;</description><category>coursera</category><category>edtech</category><category>google</category><category>privacy</category><guid>http://paulolivier.dehaye.org/posts/edtech-policies-part-i.html</guid><pubDate>Tue, 09 Sep 2014 10:23:58 GMT</pubDate></item><item><title>How fast the world has changed</title><link>http://paulolivier.dehaye.org/posts/how-fast-the-world-has-changed.html</link><dc:creator>Paul-Olivier Dehaye</dc:creator><description>&lt;p&gt;As I am starting to blog, I am taking a course on &lt;a class="reference external" href="http://schoolofopen.p2pu.org/"&gt;openness with P2PU&lt;/a&gt;. This is helping me think of how to quickly and most efficiently &lt;a class="reference external" href="http://opencontent.org/blog/archives/3393"&gt;reclaim my domain&lt;/a&gt;. For this, I have been looking back at the few notes I have posted around on Facebook. Actually, you can too if you have a Facebook account, as I have just made them &lt;a class="reference external" href="https://www.facebook.com/paulolivier.dehaye/notes"&gt;viewable to every facebook user&lt;/a&gt;. This way, if you are new here, you can get a quick sense of what I am about. Incidentally, due to a little known item in the Facebook &lt;a class="reference external" href="https://www.facebook.com/about/privacy/your-info-on-fb"&gt;privacy policy&lt;/a&gt;, this means that all the comments are now public as well (i.e. my friends gave the rights to their comments to Facebook, I gave the right to my original posts to Facebook, and finally Facebook decides to correlate privacy of comments to the privacy settings of the original post). I suppose every platform owner has to make lots of those decisions, which eventually shape the service.&lt;/p&gt;
&lt;p&gt;In any case, one of the notes struck me. It was just a link, actually, to a 2008 &lt;a class="reference external" href="http://warner.blogs.nytimes.com/2008/12/04/first-the-bad-news/"&gt;New York Times story&lt;/a&gt; about an unfortunate Walmart employee who was trampled to death on Black Friday. The angle is that bad things happen in the world, and sometimes this can end up on the internet, &lt;a class="reference external" href="https://www.youtube.com/watch?v=7aUwmsi6Wc0"&gt;filmed on crappy cell phone cameras&lt;/a&gt;. And then we have to tell kids about all that violence. The undertone of the piece is that we were just starting to grapple with that problem back then. A friend of mine commented and asked for my opinion. The original note is &lt;a class="reference external" href="https://www.facebook.com/notes/paul-olivier-dehaye/kids-are-scary-sometimes/103572210077"&gt;here&lt;/a&gt;, with a contemporary screenshot below.&lt;/p&gt;
&lt;img alt="../walmart.jpg" class="align-center" src="http://paulolivier.dehaye.org/walmart.jpg" style="width: 629.0px; height: 307.0px;"&gt;&lt;p&gt;Close to six years later, we are still facing the same problems, of course. Internet is more &lt;a class="reference external" href="http://www.mercurynews.com/business/ci_26459616/youtube-twitter-teamed-halt-spread-beheading-video-report"&gt;violent&lt;/a&gt; and more &lt;a class="reference external" href="http://www.forbes.com/sites/emmawoollacott/2014/09/08/reddit-gives-mixed-messages-after-pulling-leaked-celebrity-photos/"&gt;invasive&lt;/a&gt; than ever. Any platform owner knows the value of good filters to curate content for its users. This content curation can be done jointly by machines and humans, leading to risks of algorithmic bias still misunderstood (&lt;a class="reference external" href="http://www.forbes.com/sites/dailymuse/2014/08/04/the-facebook-experiment-what-it-means-for-you/"&gt;Facebook&lt;/a&gt;, &lt;a class="reference external" href="https://medium.com/message/the-algorithm-giveth-but-it-also-taketh-b7efad92bc1f"&gt;Twitter&lt;/a&gt;). It can also be done exclusively by humans, operating under strict rules. For both posts and comments. This is the model applied by &lt;a class="reference external" href="http://www.metafilter.com"&gt;metafilter&lt;/a&gt;, leading to high quality output but a relatively weak business model, unfortunately &lt;a class="reference external" href="http://gigaom.com/2014/05/22/if-a-high-quality-site-like-metafilter-can-be-crushed-by-google-what-hope-do-other-sites-have/"&gt;still vulnerable to algorithmic whims&lt;/a&gt;.&lt;/p&gt;
&lt;blockquote class="epigraph"&gt;
&lt;dl class="docutils"&gt;&lt;dt&gt;So you're saying that people tried to use the economies of scale of the internet to disrupt the conventional and somewhat hidebound traditional methods and then it turns out that certain things requiring human eyeballs and judgment do not actually scale along with this stuff and the lesson is that you need to keep people in the mix in not just token ways even if this interferes with your bottom line...? I know that song!&lt;/dt&gt;
&lt;dd&gt;-- Metafilter user and former moderator &lt;a class="reference external" href="http://www.metafilter.com/user/292"&gt;jessamyn&lt;/a&gt;&lt;/dd&gt;
&lt;/dl&gt;&lt;/blockquote&gt;
&lt;p&gt;In any case, eight years after this Facebook note, the world still turns around. People get married, have babies, raise their children. And most people still use Facebook.&lt;/p&gt;</description><category>baby</category><category>facebook</category><category>metafilter</category><category>privacy</category><category>reclaim</category><category>violence</category><category>whyopen</category><guid>http://paulolivier.dehaye.org/posts/how-fast-the-world-has-changed.html</guid><pubDate>Mon, 08 Sep 2014 13:37:35 GMT</pubDate></item><item><title>"Don't be evil", or how I learned to behave like a startup and love the data</title><link>http://paulolivier.dehaye.org/posts/dont-be-evil-or-how-i-learned-to-behave-like-a-startup-and-love-the-data.html</link><dc:creator>Paul-Olivier Dehaye</dc:creator><description>&lt;img alt="../strangelove.png" class="align-right" src="http://paulolivier.dehaye.org/strangelove.png" style="width: 320.0px; height: 240.0px;"&gt;&lt;p&gt;When Gmail was opened in 2004, I received invitations early. If I remember well, they came from a friend working at Google who had already snatched a few fun login names. I did the same, and passed on further invitations to my brother and our friends back home.&lt;/p&gt;
&lt;p&gt;A year or so later, when my brother was visiting with his friends, we went on a tour of the Googleplex. Randomly passing in front of the cubicle of a homonym, one of the friends suddenly realised why he had not been able to register his own name earlier. In other words, &lt;strong&gt;an unknown collision in the physical world had first manifested digitally&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;I like to think of those collisions as the digital equivalent of New York overcrowding, trying to fit too many people in just a few login characters.&lt;/p&gt;
&lt;p&gt;So which fun pseudonyms did we chose? Which did we consider worthy in this land grab? Certainly many of them were aimed at our shared cultural backgrounds as Belgians in the Silicon Valley. If you had &lt;a class="reference external" href="mailto:tintin@gmail.com"&gt;tintin@gmail.com&lt;/a&gt;, or &lt;a class="reference external" href="http://nl.wikipedia.org/wiki/Frietkot"&gt;frietkot@gmail.com&lt;/a&gt; that would be pretty impressive, no? Indeed, we grabbed names of regions, superheroes, movie stars, concepts, etc. We certainly thought this was OK, and didn't reflect more on something that &lt;a class="reference external" href="http://en.wikipedia.org/wiki/Nymwars"&gt;became controversial only later&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;One of the logins I grabbed had the name of a Belgian politician, let's call him Some Guy. He was on TV and I thought my friends would get a chuckle if I emailed them from it. Certainly, I might have crossed a moral line already then, but it felt like a very tiny escalation in this virtual land grab.&lt;/p&gt;
&lt;p&gt;What did I do with this account? I mostly used it for spam protection. I set it up so that all emails sent there would be forwarded to my default inbox, and gave this address whenever there was a need to a register for a spammy online service. This worked well, possibly because Gmail's algorithms had learned to weigh emails transiting through this address differently and benefited from the additional segmenting.&lt;/p&gt;
&lt;p&gt;Around 2008, inevitably, I started receiving emails addressed to That Guy. Those collisions happen to all of us, for all of our email accounts. What is the moral thing to do there? My philosophy is most of the time to let it drop, but  sometimes also to reply to the sender telling them that they got the wrong address (due to emails missent to my main account, I must have had to contact a dozen hotels in Quebec by now). In most cases, the only way to know what to do is to read the email, slightly invading this other persons' privacy.&lt;/p&gt;
&lt;blockquote&gt;
Just like Rachel and her Friends in their New York apartment, we struggle to deal with those privacy collisions, especially when we feel a need to intervene.&lt;/blockquote&gt;
&lt;iframe width="900" height="600" src="//www.youtube.com/embed/tYn8s0_kDUw?rel=0&amp;amp;hd=1&amp;amp;wmode=transparent"&gt;&lt;/iframe&gt;&lt;p&gt;For That Guy, it was even easier to feel morally OK about it: I never actively sought the emails, had no way to prevent the mistake, and anyways the emails were from cranks. On top, by that time I had registered to too many services with that pseudonym, which effectively tied my identity to it, with no way to revert the situation. So in effect this data collection was happening, whether I liked it or not, or at least that was my moral justification.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The problem with data is that it leaks.&lt;/strong&gt; The cranks don't just email one influential person at a time. They email a few, who are susceptible to know each other. As a consequence, in this case, the cranks polluted those recipients' email software  with a wrong email address. Of course, in due time, the email autocompletion software of those recipients started tripping them and I started receiving emails from other politicians to That Guy. &lt;strong&gt;Algorithmic curation had gone wrong, and actively mislead humans.&lt;/strong&gt; The fact that these were politicians might have mislead me: I should have made the effort of explaining the awkward situation to That Guy's interlocutors and tried to correct it. But I didn't. Somehow a couple more emails made it to me that were clearly of more social nature. Again, I didn't do anything. &lt;strong&gt;This data will not disappear unless actively deleted&lt;/strong&gt;, and even then I can only be so sure.&lt;/p&gt;
&lt;blockquote&gt;
At this point you will deservedly think that I am a moron. But was it morally wrong? And when exactly did it go wrong?&lt;/blockquote&gt;
&lt;p&gt;Throughout my moral justification was that I was not actively seeking this. Emails would land in my mailbox and I would have to read them to know what to do. Of course, this conveniently ignores what I could have done to prevent those emails to arrive in the first place. Part of my justification was that I wasn't doing anything with the data collected. There was no clear goal, except &lt;strong&gt;awareness that this could be used to make a point later, which I guess I am making here now publicly&lt;/strong&gt; (in fact, I have used this to make this point in private throughout the years).&lt;/p&gt;
&lt;p&gt;The more interesting issue here is to understand that this is exactly how many big data companies function. "Don't be evil" Google gobbles data all over the place &lt;a class="reference external" href="http://www.wired.com/2014/04/threatlevel_0401_streetview/"&gt;for purposes that are not always clear at the time&lt;/a&gt;, and the justification is often that this was incidental, automated and did not require human intervention. Looking at a corporate setting elevates the stakes, and my feeble moral justifications are not sufficient anymore. It becomes a matter of ethics, which arguably should be that data collection is by default unethical: data should not be kept beyond the time necessary for its intended use, with that use itself subject to precise and established ethical rules. It looks like Google has understood this in some markets, for instance education (unlike other players there), and this will be the topic of a later post.&lt;/p&gt;
&lt;p&gt;(Image in the public domain: the Dr Stangelove War Room, which &lt;a class="reference external" href="http://valleywag.gawker.com/airbnbs-office-has-a-replica-of-the-dr-strangelove-wa-1475788543"&gt;happens to be replicated in the Airbnb HQ&lt;/a&gt;)&lt;/p&gt;</description><category>ethics</category><category>privacy</category><guid>http://paulolivier.dehaye.org/posts/dont-be-evil-or-how-i-learned-to-behave-like-a-startup-and-love-the-data.html</guid><pubDate>Mon, 08 Sep 2014 09:45:51 GMT</pubDate></item></channel></rss>