<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>paulolivier.dehaye.org (ethics)</title><link>http://paulolivier.dehaye.org/</link><description></description><language>en</language><lastBuildDate>Mon, 29 Sep 2014 13:40:12 GMT</lastBuildDate><generator>http://getnikola.com/</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Learning, working and ?</title><link>http://paulolivier.dehaye.org/posts/learning-working-and.html</link><dc:creator>Paul-Olivier Dehaye</dc:creator><description>&lt;p&gt;A lot of my recent thoughts have turned around issues of crowdsourcing and online education.&lt;/p&gt;
&lt;img alt="../worklearn.jpg" class="align-right" src="http://paulolivier.dehaye.org/worklearn.jpg" style="width: 346.4px; height: 85.6px;"&gt;&lt;p&gt;One of my co-authors (&lt;a class="reference external" href="http://hci.uni-hannover.de/people/markus"&gt;Markus Krause&lt;/a&gt;) is co-organising a workshop on that topic, &lt;a class="reference external" href="http://www.worklearn.org/"&gt;WorkLearn 2014&lt;/a&gt;. It will take place in Pittsburgh November 2-4, as part of the &lt;a class="reference external" href="http://www.humancomputation.com/2014/"&gt;Human Computation conference HCOMP2014&lt;/a&gt;. Human Computation is another expression interchangeable for &lt;a class="reference external" href="http://paulolivier.dehaye.org/posts/social-teaching-machines.html"&gt;social machine&lt;/a&gt;, although it has different connotations.&lt;/p&gt;
&lt;p&gt;The stated motivation of the workshop is very ambitious:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The online education and crowdsourcing communities are addressing similar problems in educating, motivating and evaluating students and workers. The online learning community succeeds in increasing the supply side of the cognitively skilled labor market, and the crowdsourcing at scale community creates a larger marketplace for cognitively skilled work.&lt;/p&gt;
&lt;p&gt;Linking online platforms for crowd work with platforms for MOOCs has the potential to: provide knowledge and training at a massive scale to contributors; collect data that identify expert skills; engage contributors in simultaneously working and learning in a social environment; and organize large communities around online courses on specific topics. These all provide new opportunities to support and deploy sophisticated algorithms for crowd learning and work.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The most successful example in this direction is of course &lt;a class="reference external" href="http://duolingo.com"&gt;Duolingo&lt;/a&gt;, which helps translate the web while using volunteer labor by language learners. If one omits the learning, the strategy there is not that different from the strategy used by my Coursera coworker &lt;a class="reference external" href="https://www.coursera.org/instructor/bernstein"&gt;Abraham Bernstein&lt;/a&gt; to translate books using &lt;a class="reference external" href="http://en.wikipedia.org/wiki/Amazon_Mechanical_Turk"&gt;Amazon Mechanical Turk&lt;/a&gt;, and indeed part of his efforts aim to design effective tools to program those social machines (with the programming language CrowdLang).&lt;/p&gt;
&lt;embed&gt;&lt;iframe width="640" height="360" src="//www.youtube.com/embed/emCABRV2cUA" frameborder="0" allowfullscreen&gt;&lt;/iframe&gt;
&lt;/embed&gt;&lt;p&gt;I have always had some qualms about the &lt;a class="reference external" href="http://florianschmidt.co/the-good-the-bad-and-the-ugly/"&gt;ethics of crowdsourcing&lt;/a&gt;, even though it can clearly be used for good: the prototypical success story is in the work of Ushahidi during the &lt;a class="reference external" href="http://www.ushahidi.com/blog/2012/01/12/haiti-and-the-power-of-crowdsourcing/"&gt;2010 Haiti earthquake&lt;/a&gt;. I was thus very happy to see over Labor Day 2014 that Michael Bernstein from Stanford announced &lt;a class="reference external" href="http://crowdresearch.org/blog/?p=9039"&gt;guidelines for academic requesters on Amazon Mechanical Turk&lt;/a&gt;. He explains the rationale for this (a Turker is a worker on a crowdsourcing platform):&lt;/p&gt;
&lt;blockquote&gt;
An IRB-approved researcher experimented on the [crowdsourcing] platform unannounced. The result was Turker confusion, strife, and wasted time, in a system where time is what it takes to make ends meet.&lt;/blockquote&gt;
&lt;p&gt;These guidelines were themselves crowdsourced, designed together with the Turkers (it's only natural!).&lt;/p&gt;
&lt;p&gt;At the same time, over the summer, there was a huge controversy over the iffy ethics of social platforms experimentation. This is due to the release at the very end of June 2014 of a &lt;a class="reference external" href="http://www.forbes.com/sites/kashmirhill/2014/06/29/facebook-doesnt-understand-the-fuss-about-its-emotion-manipulation-study/"&gt;Facebook experiment on its users&lt;/a&gt; (don't miss the Cornell IRB flowchart there). There are a ton of links about this, but the best is probably the account by Mary L. Gray of an &lt;a class="reference external" href="http://marylgray.org/?page_id=203"&gt;ethics panel that took place at the Microsoft Research Faculty Summit&lt;/a&gt; (and unfortunately was published with much delay).&lt;/p&gt;
&lt;p&gt;In any case, this should give serious pause to any educator. One can see lots of fields suddenly getting much too close, with very different or inexistent values. Online learners, just as Turkers, are vulnerable. &lt;a class="reference external" href="http://nogoodreason.typepad.co.uk/no_good_reason/2014/06/the-ethics-of-digital-scholarship.html"&gt;Martin Weller&lt;/a&gt; and &lt;a class="reference external" href="http://www.elearnspace.org/blog/2014/01/13/the-vulnerability-of-learning/"&gt;George Siemens&lt;/a&gt; have recently insisted on this.&lt;/p&gt;
&lt;p&gt;So, what do you think? Anyone wants to submit a position paper (2 pages) on the topic? Any of my co-learners in MOOCs would like to see what we can do? We could, well... crowdsource it...&lt;/p&gt;
&lt;p&gt;(of course, this was due yesterday: official deadline is "September")&lt;/p&gt;</description><category>connected_course</category><category>coursera</category><category>crowdsourcing</category><category>duolingo</category><category>ethics</category><category>t509massive</category><category>whyopen</category><guid>http://paulolivier.dehaye.org/posts/learning-working-and.html</guid><pubDate>Thu, 11 Sep 2014 00:17:10 GMT</pubDate></item><item><title>"Don't be evil", or how I learned to behave like a startup and love the data</title><link>http://paulolivier.dehaye.org/posts/dont-be-evil-or-how-i-learned-to-behave-like-a-startup-and-love-the-data.html</link><dc:creator>Paul-Olivier Dehaye</dc:creator><description>&lt;img alt="../strangelove.png" class="align-right" src="http://paulolivier.dehaye.org/strangelove.png" style="width: 320.0px; height: 240.0px;"&gt;&lt;p&gt;When Gmail was opened in 2004, I received invitations early. If I remember well, they came from a friend working at Google who had already snatched a few fun login names. I did the same, and passed on further invitations to my brother and our friends back home.&lt;/p&gt;
&lt;p&gt;A year or so later, when my brother was visiting with his friends, we went on a tour of the Googleplex. Randomly passing in front of the cubicle of a homonym, one of the friends suddenly realised why he had not been able to register his own name earlier. In other words, &lt;strong&gt;an unknown collision in the physical world had first manifested digitally&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;I like to think of those collisions as the digital equivalent of New York overcrowding, trying to fit too many people in just a few login characters.&lt;/p&gt;
&lt;p&gt;So which fun pseudonyms did we chose? Which did we consider worthy in this land grab? Certainly many of them were aimed at our shared cultural backgrounds as Belgians in the Silicon Valley. If you had &lt;a class="reference external" href="mailto:tintin@gmail.com"&gt;tintin@gmail.com&lt;/a&gt;, or &lt;a class="reference external" href="http://nl.wikipedia.org/wiki/Frietkot"&gt;frietkot@gmail.com&lt;/a&gt; that would be pretty impressive, no? Indeed, we grabbed names of regions, superheroes, movie stars, concepts, etc. We certainly thought this was OK, and didn't reflect more on something that &lt;a class="reference external" href="http://en.wikipedia.org/wiki/Nymwars"&gt;became controversial only later&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;One of the logins I grabbed had the name of a Belgian politician, let's call him Some Guy. He was on TV and I thought my friends would get a chuckle if I emailed them from it. Certainly, I might have crossed a moral line already then, but it felt like a very tiny escalation in this virtual land grab.&lt;/p&gt;
&lt;p&gt;What did I do with this account? I mostly used it for spam protection. I set it up so that all emails sent there would be forwarded to my default inbox, and gave this address whenever there was a need to a register for a spammy online service. This worked well, possibly because Gmail's algorithms had learned to weigh emails transiting through this address differently and benefited from the additional segmenting.&lt;/p&gt;
&lt;p&gt;Around 2008, inevitably, I started receiving emails addressed to That Guy. Those collisions happen to all of us, for all of our email accounts. What is the moral thing to do there? My philosophy is most of the time to let it drop, but  sometimes also to reply to the sender telling them that they got the wrong address (due to emails missent to my main account, I must have had to contact a dozen hotels in Quebec by now). In most cases, the only way to know what to do is to read the email, slightly invading this other persons' privacy.&lt;/p&gt;
&lt;blockquote&gt;
Just like Rachel and her Friends in their New York apartment, we struggle to deal with those privacy collisions, especially when we feel a need to intervene.&lt;/blockquote&gt;
&lt;iframe width="900" height="600" src="//www.youtube.com/embed/tYn8s0_kDUw?rel=0&amp;amp;hd=1&amp;amp;wmode=transparent"&gt;&lt;/iframe&gt;&lt;p&gt;For That Guy, it was even easier to feel morally OK about it: I never actively sought the emails, had no way to prevent the mistake, and anyways the emails were from cranks. On top, by that time I had registered to too many services with that pseudonym, which effectively tied my identity to it, with no way to revert the situation. So in effect this data collection was happening, whether I liked it or not, or at least that was my moral justification.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The problem with data is that it leaks.&lt;/strong&gt; The cranks don't just email one influential person at a time. They email a few, who are susceptible to know each other. As a consequence, in this case, the cranks polluted those recipients' email software  with a wrong email address. Of course, in due time, the email autocompletion software of those recipients started tripping them and I started receiving emails from other politicians to That Guy. &lt;strong&gt;Algorithmic curation had gone wrong, and actively mislead humans.&lt;/strong&gt; The fact that these were politicians might have mislead me: I should have made the effort of explaining the awkward situation to That Guy's interlocutors and tried to correct it. But I didn't. Somehow a couple more emails made it to me that were clearly of more social nature. Again, I didn't do anything. &lt;strong&gt;This data will not disappear unless actively deleted&lt;/strong&gt;, and even then I can only be so sure.&lt;/p&gt;
&lt;blockquote&gt;
At this point you will deservedly think that I am a moron. But was it morally wrong? And when exactly did it go wrong?&lt;/blockquote&gt;
&lt;p&gt;Throughout my moral justification was that I was not actively seeking this. Emails would land in my mailbox and I would have to read them to know what to do. Of course, this conveniently ignores what I could have done to prevent those emails to arrive in the first place. Part of my justification was that I wasn't doing anything with the data collected. There was no clear goal, except &lt;strong&gt;awareness that this could be used to make a point later, which I guess I am making here now publicly&lt;/strong&gt; (in fact, I have used this to make this point in private throughout the years).&lt;/p&gt;
&lt;p&gt;The more interesting issue here is to understand that this is exactly how many big data companies function. "Don't be evil" Google gobbles data all over the place &lt;a class="reference external" href="http://www.wired.com/2014/04/threatlevel_0401_streetview/"&gt;for purposes that are not always clear at the time&lt;/a&gt;, and the justification is often that this was incidental, automated and did not require human intervention. Looking at a corporate setting elevates the stakes, and my feeble moral justifications are not sufficient anymore. It becomes a matter of ethics, which arguably should be that data collection is by default unethical: data should not be kept beyond the time necessary for its intended use, with that use itself subject to precise and established ethical rules. It looks like Google has understood this in some markets, for instance education (unlike other players there), and this will be the topic of a later post.&lt;/p&gt;
&lt;p&gt;(Image in the public domain: the Dr Stangelove War Room, which &lt;a class="reference external" href="http://valleywag.gawker.com/airbnbs-office-has-a-replica-of-the-dr-strangelove-wa-1475788543"&gt;happens to be replicated in the Airbnb HQ&lt;/a&gt;)&lt;/p&gt;</description><category>ethics</category><category>privacy</category><guid>http://paulolivier.dehaye.org/posts/dont-be-evil-or-how-i-learned-to-behave-like-a-startup-and-love-the-data.html</guid><pubDate>Mon, 08 Sep 2014 09:45:51 GMT</pubDate></item><item><title>Keeping a Soul in the Driver's Seat</title><link>http://paulolivier.dehaye.org/posts/keeping-a-soul-in-the-driver-seat.html</link><dc:creator>Paul-Olivier Dehaye</dc:creator><description>&lt;img alt="../railway.jpg" class="align-right" src="http://paulolivier.dehaye.org/railway.jpg"&gt;&lt;p&gt;I can't wait for driverless cars. Ten years is the estimate. Combined with car sharing, they will revolutionise our cities and make them much more efficient and livable. They should bring about umitigated good to our society. Yet they still come with ethical challenges, usually categorized as &lt;a class="reference external" href="http://en.wikipedia.org/wiki/Trolley_problem"&gt;trolley problems&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Wired just published an article called &lt;a class="reference external" href="http://www.wired.com/2014/08/heres-a-terrible-idea-robot-cars-with-adjustable-ethics-settings/"&gt;Here's a Terrible Idea: Robot Cars With Adjustable Ethics Settings&lt;/a&gt;, outlining the ethical issues involved in substituting human drivers with robots.&lt;/p&gt;
&lt;blockquote&gt;
In freak accidents, computers would have to take decisions such as &lt;strong&gt;killing one motorcyclist without a helmet vs. killing five pedestrians&lt;/strong&gt;.&lt;/blockquote&gt;
&lt;p&gt;The writer raises many such dystopian choices: &lt;strong&gt;children vs elderly&lt;/strong&gt;, &lt;strong&gt;us vs others&lt;/strong&gt;, &lt;strong&gt;rich vs poor&lt;/strong&gt;, etc. He rightfully sees a liability for anyone having to program those decisions. In his opinion, any attempt by the car manufacturer to distantiate itself from lawsuits by offering variable ethical settings to the owner of the car would not decrease the liability of the car manufacturer, and therefore this remains an obstacle to rolling in a driverless car.&lt;/p&gt;
&lt;blockquote&gt;
The car company has another option, which is missed by the writer: absorb progressively the insurance business.&lt;/blockquote&gt;
&lt;p&gt;First off, it's clear that any level of indirection and legal tangling is helpful in freak legal confrontations to shield car manufacturer from legal responsibility towards private individuals. Secondly, the writer does not give enough credit to the creativity of engineers/lawyers/business types.&lt;/p&gt;
&lt;blockquote&gt;
Why wouldn't they be able to introduce one further level of indirection? The manufacturer could build a "car without a soul".&lt;/blockquote&gt;
&lt;p&gt;The car could offer full access proprietary APIs to its raw or slightly processed data, but require linking to an ethical core library before it would start. This ethical core would only be called upon if a future collision is detected, and asked to respond to the really tough questions (or it could be run on a loop validating any driving input). Who would take the liability of writing such a core? Insurance companies would seem like the natural candidate. In fact, this is a very natural extension of their business, litigating for the choices they have coldly programmed in rather than the mistakes made by their clients. It would also make sense to decentralise geographically this ethical core, since driving customs are bound to vary from country to country (think of these &lt;a class="reference external" href="https://www.youtube.com/watch?v=7vd_OuqUAaI"&gt;comparatively safe Indian drivers&lt;/a&gt; or this &lt;a class="reference external" href="https://www.youtube.com/watch?v=RjrEQaG5jPM"&gt;Russian ninja&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;The question is whether insurance companies would be willing to go along. They would certainly feel pressure to adapt to a world of driverless cars, but the brilliant move for the car company would be to promise increased efficiency and reach to the whole insurance industry (more clients), and act as a middleman. By encouraging collaboration between the insurance companies, ostensibly to help them save money on R&amp;amp;D, standardise good practice, exchange regulatory tips, etc, the car company would crowdsource the insurance industry to force itself into obsolescence. This would allow the car company to eventually provide the full product, once all the R&amp;amp;D costs of the fine tuning of the ethical core have been shouldered by the insurance companies. Note that this core would only be ethical in name, as it would have been exclusively fine tuned with cost efficiency in mind.&lt;/p&gt;
&lt;p&gt;This assumes there is a car company that is sufficiently dominating the car industry to strong arm insurance companies.&lt;/p&gt;
&lt;p&gt;(For other futuristic and "fun" questions on the transformation brought about by driverless cars, see the amazingly cold-blooded &lt;a class="reference external" href="http://fortune.com/2014/08/15/if-driverless-cars-save-lives-where-will-we-get-organs/"&gt;If driverless cars save lives, where will we get organs?&lt;/a&gt;.)&lt;/p&gt;
&lt;p&gt;(Image credit: Wikipedia)&lt;/p&gt;</description><category>crowdsourcing</category><category>ethics</category><guid>http://paulolivier.dehaye.org/posts/keeping-a-soul-in-the-driver-seat.html</guid><pubDate>Fri, 22 Aug 2014 09:16:37 GMT</pubDate></item></channel></rss>