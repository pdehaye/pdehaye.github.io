<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>paulolivier.dehaye.org (ethics)</title><link>http://paulolivier.dehaye.org/</link><description></description><language>en</language><lastBuildDate>Sat, 25 Oct 2014 04:02:28 GMT</lastBuildDate><generator>http://getnikola.com/</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Extended statement on #massiveteaching (part II)</title><link>http://paulolivier.dehaye.org/posts/extended-statement-on-massiveteaching-part-ii.html</link><dc:creator>Paul-Olivier Dehaye</dc:creator><description>&lt;p&gt;(This post is meant to be an account what happened in the Coursera course &lt;em&gt;Massive Teaching: New skills required&lt;/em&gt;. I suggest reading the &lt;a class="reference external" href="http://paulolivier.dehaye.org/posts/short-statement-on-massiveteaching-part-i.html"&gt;previous post&lt;/a&gt; first. This honors my commitment to trying to get the truth to my students)&lt;/p&gt;
&lt;div class="section" id="why-the-course"&gt;
&lt;h2&gt;Why the course?&lt;/h2&gt;
&lt;p&gt;I do believe that MOOCs offer many opportunities. Beyond providing educational material at a very large scale, they might for instance also help strengthen democracy or enable new discoveries. I have &lt;a class="reference external" href="http://paulolivier.dehaye.org/posts/mooc-research-proposal-using-crowdsourcing.html"&gt;submitted a grant proposal along those lines&lt;/a&gt;. Since utopian ideas often get compromised and ultimately shaped by commercial interests, I started in May 2013 to take a closer look at contracts between MOOC providers and educational institutions. Coursera was a natural choice for my focus: my university has partnered with them, they use closed source software (which increases lock-in), and they are the leading MOOC provider right now.&lt;/p&gt;
&lt;p&gt;What I read really bothered me, for a wide array of reasons. While the utopian vision of MOOCs (free-education-for-all) is compelling, these contracts reflect a very disturbing approach to that goal. In my opinion these contracts will have large societal implications, should they become the norm: disappearance of academic freedom, of the free agency of students, and complete disappearance of any form of privacy when learning. When I tried to communicate this around me, I encountered a clear lack of information on the subject. Getting past the utopian vision took time. Once there, many responded that my concerns were valid and worth discussing. In any case, this approach was ineffective: MOOC partnership decisions are often made by university administrations, based on factors other than those that drove my concerns.&lt;/p&gt;
&lt;p&gt;I decided that the best way to proceed was to teach a MOOC on the topic. The most natural place to do so was of course Coursera, and it also seemed natural to address the course to professors, i.e. individuals who might be able to affect decisions at their own institutions or in their local communities. It would also allow me to reach out to the more "typical" Coursera students and explain to them the contracts that are constructed around their learning. Finally, I thought it would be interesting to see the capacities at Coursera to reflect on their own practices.&lt;/p&gt;
&lt;p&gt;I understand the paradox of teaching on Coursera about concerns relating to teaching on Coursera, but made the calculation that the benefits outweigh the costs in this case.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="timeline-of-preparation-and-structure-of-the-course"&gt;
&lt;h2&gt;Timeline of preparation and structure of the course&lt;/h2&gt;
&lt;p&gt;The course was approved by Coursera mid-April. The Quality Assurance Protocol at Coursera requires the first two weeks of material to be uploaded ahead of the beginning of the course. The third week (and later) can be uploaded at the last minute.&lt;/p&gt;
&lt;p&gt;In consequence, I structured the course &lt;a class="reference external" href="https://www.youtube.com/watch?v=1xnBH0JDaU8"&gt;in the following way&lt;/a&gt;:&lt;/p&gt;
&lt;ul class="simple"&gt;&lt;li&gt;week 1: pedagogy&lt;/li&gt;
&lt;li&gt;week 2: technology and copyright&lt;/li&gt;
&lt;li&gt;week 3: business model, science fiction (utopia/dystopia), unresolved questions&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;In other words, the first two weeks were conceived to provide background for the third week, which would be quite critical of MOOC contracts. I didn't hide from Coursera or the students that I would be critical, and used the flexibility offered by Coursera not to provide the third week material ahead of time. There were several reasons for this:&lt;/p&gt;
&lt;ul class="simple"&gt;&lt;li&gt;The MOOC world changes fast. In the three weeks before the course started, two comprehensive reports came out on MOOCs. I also &lt;a class="reference external" href="http://paulolivier.dehaye.org/posts/what-i-am-doing-with-moocs-and-why-june-2nd.html"&gt;attended two conferences on MOOCs, and was co-organiser of one&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;I knew there was a risk that Coursera would shut down the course, either directly or indirectly. Coursera did drag their feet more and more in the lead up to the course. I attribute that to the realisation of the extent of what I wanted to say, but they could easily chalk it to other reasons.&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;I conceived simple videos to illustrate concepts (like an illustrated glossary), but did not mean for this to constitute the whole course. The videos would be accompanied by readings and forum interactions. Of course, I recognised that students could still desire different levels of interactions, and I was fine with this diversity. Also, this course was meant to be &lt;a class="reference external" href="https://www.youtube.com/watch?v=1xnBH0JDaU8"&gt;very responsive to evolutions in the MOOC world in its later runs&lt;/a&gt;, which explains why the videos are relatively condensed and interchangeable.&lt;/p&gt;
&lt;p&gt;Because of the openness of the topic, I tried as much as possible not to give the impression of a "full course". This meant that it did not deliver either a Statement of Accomplishment or a Verified Certificate. Due to the Coursera contracts, I was still required to deliver a numerical grade at the end (which couldn't be 0% for all or 100% for all). I tentatively settled for a bland peer-feedback assessment, as I thought this would be more useful than a multiple choice quiz.&lt;/p&gt;
&lt;p&gt;In addition, I prepared some "experiments" through the course forums. The term is very confusing: these were not scientific or social experiments, but clearly advertised and explained attempts to talk with the students, get the students to engage with the content and interact with each other and advance the state-of-the-art in collaborative tools in MOOCs. Unfortunately I have now lost electronic access to the course material itself, so I cannot relay the exact wording used to explain this to the students.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="first-week-of-the-course"&gt;
&lt;h2&gt;First week of the course&lt;/h2&gt;
&lt;p&gt;The first week of the course went according to me quite well, although it was exhausting: I interacted and welcomed dozens of students on the forum, orientated them, etc.&lt;/p&gt;
&lt;p&gt;In addition, I participated with the students in the "experiments" that I had prepared:&lt;/p&gt;
&lt;ul class="simple"&gt;&lt;li&gt;I had defined a process to manage the forums based on their preferences (like &lt;a class="reference external" href="http://area51.stackexchange.com/"&gt;Area 51 on Stack Exchange&lt;/a&gt;), but this had to be scrapped (lack of interest) and turned instead into a listing of desired features in forums;&lt;/li&gt;
&lt;li&gt;One student initiated a wide survey of MOOC students by MOOC students, that I encouraged;&lt;/li&gt;
&lt;li&gt;I decided to attempt to build a badging system (Open Badges is an infrastructure that allows the cryptographically secure attribution of rewards for contributions and achievements, however small or big). This was done in response to Coursera's requirement for a grade, and because I thought it would be very effective at shifting power towards the students/professors relationship rather than the universities/MOOC platforms. In my mind these badges would be based around the forums, and aim to make the final outcome more meaningful to each individual student (via student-defined badges granted through peer-feedback and certified by the instructor). This could have been achieved in this iteration or more realistically in a later iteration of the course, based on interactions already happening in the first run. In any case, this required looking a bit deeper into the kind of data collected on students (or at least the fraction of that data that was accessible to me as an instructor). To give an example, I would need to be able to see how an upvote was encoded, and whether there would be a way to retrieve useful information from that. I asked Coursera for an anonymized database dump taken &lt;em&gt;before&lt;/em&gt; the course properly started, with some forums seeded with test data (generated by me and another instructor account). When I asked to share parts of this dump with the students (even in a sanitized version with synthetic data), in the interest of transparency, it was refused. Throughout Coursera dragged their feet, which only encouraged me to dig deeper: I was also forbidden to share the supporting documentation PDFs, and my request to share those refusal emails with the students was never answered.&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;During the first week, some students also started Facebook and Google Plus groups associated to the course, unprompted.&lt;/p&gt;
&lt;p&gt;From the start of the course, I also used confusion as a teaching tool, by sometimes acting a bit randomly but in non obtrusive ways. This was done to expose hidden assumptions, and incite reactions, which in a MOOC would be numerous and were likely to be contradictory and diverse. I saw this as conducive to questioning of the instructor himself (i.e. me, an individual having signed contracts unknown to the students) and therefore to exploratory learning, especially with my intended audience of professors. I realise this is quite bold, but I merely saw this as an extreme transposition to the web of the Socratic method. Since this concerned purely my teaching methods and did not involve any research component, I did not make a formal announcement about this, but was overt at the start, for instance by opening a thread with title "?". This lead one student to answer 42, then another to give an equation. Other students wrote poems.&lt;/p&gt;
&lt;p&gt;I would love to debate the value of this technique, particularly when the goal is partly to push students to turn a critical eye towards an opaque and complex legal and technical construction such as Coursera, under various restrictions on the content. Bear also in mind that students were at any point free to leave. Some certainly did but many stayed. I was also hoping that the combinatorics of the peer-feedback exercise at the end would tie loose ends at scale and cement the efficacy of the technique (since it is my suspicion that a comment such as "I am still confused by..." is more likely to elicit constructive responses from peers than "I assert this and that...").&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="second-week-of-the-course"&gt;
&lt;h2&gt;Second week of the course&lt;/h2&gt;
&lt;p&gt;Social aspects of learning are currently completely unstructured and relatively weak on MOOC platforms, so a natural next step for them is to build some form of social network for students. Whatever form it takes (decentralised around each course, for instance), it will require extensive research unlikely to be done by the university partners since it will be core to the MOOC platforms. Concerning research practices, Facebook and Coursera have very similar, open-ended, Terms of Use.&lt;/p&gt;
&lt;p&gt;During the first weekend of the course (June 29th?), the Facebook Emotion study made news: Facebook had manipulated the newsfeeds of many users, trying to selectively induce either happiness or sadness. The news coverage was extremely confusing: many academics were outraged at the lack of IRB approval, while others were unsurprised at these commercial practices.&lt;/p&gt;
&lt;p&gt;Having read Coursera Terms of Use, I knew right away that similar abuses could take place there, and had many reasons to think not enough safeguards were in place at Coursera either. I very quickly saw the &lt;a class="reference external" href="http://paulolivier.dehaye.org/posts/erosion-of-thick-legitimacy-by-coursera.html"&gt;pernicious threat that Coursera's business model and practices represent to the thick legitimacy of instructors, researchers and universities&lt;/a&gt;, and ultimately to society.&lt;/p&gt;
&lt;p&gt;Over and above this, I knew that data collected on the Coursera platform has no expiration date, can be replayed at will and that it had not been welcoming of my own transparency effort. In that sense, there was a more immediate concern, towards my students.&lt;/p&gt;
&lt;p&gt;Unlike all the material I had prepared for week 3, which was researched and based on documents available to the general public (i.e. not derived from the Quality Assurance process, the Coursera Partners' Portal or private communication), any comment of mine linking ethics of experimenting at Coursera to ethics of experimenting at Facebook would have to be speculative at the time. Making this speculation public would expose me to the risk of a legal challenge. Continuing the course would put more ethical responsibility on me since I could not be fully transparent with the students, as I had been thus far. To add to all of this, all the unresolved ethical questions led me to question the wisdom of implementing the badging experimentation within Coursera itself.&lt;/p&gt;
&lt;p&gt;I started questioning the ethics of delivering a course in those conditions. I could not escape thinking along the lines of nested Stanford Experiment (Should I promote some students to Community TA? What are the other 599 courses doing?). Whatever I could think of doing I somehow could find a darker side as well, associating it to one of the studies that I had read in preparing the course. I became confused, but at a much deeper level: under all these adverse conditions, I should probably have decided to stop the course, even if doing so exposed me to legal risk. Instead, I pressed on, and resolved to deliver the rest of the course via Twitter and Youtube instead (while the willing students would support each other through the Facebook and Google Plus groups as well). I made no decision about the third week peer-feedback exam. Of course, in doing all this, I also confused the students to this much deeper level.&lt;/p&gt;
&lt;p&gt;For somewhere between 24 and 48 hours, I improvised and set out to prepare material on Twitter to support my upcoming explanations. Some of it was also intentionally confounding, likely to be misunderstood by anyone who was not involved in the course, but easily explainable with the proper context (Coursera repeatedly ignored requests to engage with me in a more public way within the course). In my (poor) judgement, confusion could be used to expose hidden assumptions through reactions of other parties. For instance, I knew this might lead Coursera to stop the course, which was fine with me.&lt;/p&gt;
&lt;p&gt;Another "subtext" of my tweets was the risk of &lt;a class="reference external" href="http://socialmediacollective.org/2014/06/26/corrupt-personalization/"&gt;corrupt personalisation&lt;/a&gt; (June 26th) in teaching, originating from &lt;a class="reference external" href="http://blogs.law.harvard.edu/niftyc/archives/975"&gt;algorithmic culture&lt;/a&gt;. I thought it would be interesting to show this to students, centered around the World Cup taking place at the same time (Twitter, for the duration of the World Cup, offered the opportunity to do your own A/B testing by selecting which team you wanted to support).&lt;/p&gt;
&lt;p&gt;Yet another plan was to continue the course as a Twitter based game, that could involve participants external to the course as well. Setting it up as a game would allow me to imply things without properly saying them, diffusing some of the legal risk away from myself.&lt;/p&gt;
&lt;p&gt;I realise these are many options, but they did not need all to work and stick. And I knew I would be more free to explain them once the course finished.&lt;/p&gt;
&lt;p&gt;When ready, on Wednesday, I removed all content and forums, except for one forum. I also pinned one student post with an encouragement to fellow students to take ownership of their learning and join the Google Plus group associated to the course (in fact, this post is what triggered me to do this at that exact time). I removed video content because I wanted to encourage students who only watch video to consult the forums and notice that something was happening beyond business as usual. At the time I still felt very wary of the final peer-feedback exercise, and did not want these students to suddenly feel cheated. I intended to explain my actions on the very limited space I had left on the Coursera forums, at the best I could within my legal constraints.&lt;/p&gt;
&lt;p&gt;As the atmosphere had evolved between Monday and Wednesday, it was clear to me that students would be critical of switching from Coursera to another web platform. Again, this was fine as I could point them to the numerous Terms of Use I had read in preparation of the course, and show them contradictions in their own reasonings about privacy, and overreliance on sales pitches. I had already started highlighting the social network aspect of Coursera on Sunday or Monday, and ultimately wanted students to treat all those options on an equal footing, and make critical choices.&lt;/p&gt;
&lt;p&gt;Both Twitter and Youtube afford advantages that were required for me to be able to continue with the course content originally planned (hyperlinking to specific second or Tweet), while not requiring any login to consume passively.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="fallout"&gt;
&lt;h2&gt;Fallout&lt;/h2&gt;
&lt;p&gt;At this stage (Wednesday), I was given a 24 hour deadline by Coursera to reinstate the content on their servers. This request arrived at 12:30 AM my time. When I woke up, I asked my university for ethical guidance through one channel.  Before I could consult with my university, by 11AM, I was removed as an instructor and Coursera engineers started reinstating content in a suspiciously selective fashion, reeking of sanctioned censorship (part of the contracts is that the instructor has to sign away rights to modify the material).&lt;/p&gt;
&lt;p&gt;Once my Coursera instructor rights were removed, in the interest of transparency, I immediately explained to the students what was going on through an etherpad document (for increased interactivity, still feeling under legal threat). Agreeing to a student request, I encouraged participants in that etherpad chat to share the document with the whole class. I also posted on the course forums and at the time I could not see that all my posts were being systematically deleted. I still do not know by whom.&lt;/p&gt;
&lt;p&gt;Confusingly, some students still received e-mail notifications of my messages, and they started to suspect I was deleting them myself. In parallel, there was increasing speculation on the blogosphere that I was performing some form of social experiment. New students were not allowed to sign up, so newcomers had little access to information about what had happened in the course itself. This fed an increased paranoia of some students against me.&lt;/p&gt;
&lt;p&gt;Despite all these misunderstandings, I was forbidden to clarify the situation with my students or the press. Later, I learned of misleading and false accusations made by Coursera to my university, and that Coursera used browsing behaviour information to support some of their claims.
I also learned that Coursera had issued legal threats against my university and was told that my own legal situation was precarious. Coursera temporarily suspended their agreement with my university. My assessment is that Coursera has so far successfully manipulated the media, my university and the students to damage my credibility and introduce doubts about my integrity.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="summary"&gt;
&lt;h2&gt;Summary&lt;/h2&gt;
&lt;p&gt;I regret the breakdown of trust that occurred during the preparation of the course between Coursera and me, since this left me with no good option when an external event (the Facebook Emotion experiment) disrupted my course plan. I think Coursera's actions after I removed the content, such as deleting my messages or misleading journalists and my university, caused undue stress to the students.&lt;/p&gt;
&lt;p&gt;I can fully understand the perspective of students who would feel cheated or manipulated in some way since so much information was hidden away from them. I tried my best to convey that a lot was going on behind the scenes (and clearly said so many times, referring explicitly to contracts and denied requests for data transparency).&lt;/p&gt;
&lt;p&gt;In the end, it feels like I have missed a chance to raise this debate and others more constructively. This is unfortunate since there are many more issues and opportunities of MOOCs I would have wanted to discuss in the third week:&lt;/p&gt;
&lt;ul class="simple"&gt;&lt;li&gt;intellectual property for professors and associated labor issues (cf. &lt;a class="reference external" href="http://ucscfa.org/2013/06/scfas-ongoing-discussion-concerning-ucscs-contract-with-coursera/"&gt;UCSC Faculty union resistance to Coursera&lt;/a&gt; );&lt;/li&gt;
&lt;li&gt;reportedly bad working conditions in the Coursera Global Translator Community, and the risks of crowdsourcing more components of teaching (Community TAs or alumni-led tutoring, for instance);&lt;/li&gt;
&lt;li&gt;"partner"-walling of information in the Coursera Partner Portal that should be public (such as general best practices around MOOCs, data practices, &lt;a class="reference external" href="http://paulolivier.dehaye.org/posts/thin-legitimacy-at-whisper-facebook-and-coursera.html"&gt;discussion around IRBs&lt;/a&gt; etc);&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://www.youtube.com/watch?v=eJovz6IVzFU"&gt;vendor lock-in vs free/open source software&lt;/a&gt;;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="http://www.worklearn.org"&gt;merging of labour and educational markets, through crowdsourcing&lt;/a&gt;;&lt;/li&gt;
&lt;li&gt;technical interoperability of MOOC certificates;&lt;/li&gt;
&lt;li&gt;corrupt personalisation of educational experiences;&lt;/li&gt;
&lt;li&gt;erosion of academic freedom on commercial MOOC platforms;&lt;/li&gt;
&lt;li&gt;interactions &lt;a class="reference external" href="http://hci.uwaterloo.ca/faculty/elaw/citizenx/program.html"&gt;between MOOCs and citizen science or open democracies projects&lt;/a&gt;;&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;Of course, I missed a chance, but any of the other 103 Coursera partners, of which 25 or so are European or Swiss, have the option of starting a course on the same topics. I think it is very much needed: the shared interest of students and academics is to discuss these issues directly together at scale, without channeling this interaction through an intermediate corporation with obvious monetary interest. It is not clear that that many students are actually aware of any of those issues, highlighting the need for an urgent intervention on the topic.&lt;/p&gt;
&lt;p&gt;(I realise I have not answered all the questions or concerns that people might have about the course. The comment section is below, feel free to use it!)&lt;/p&gt;
&lt;/div&gt;</description><category>coursera</category><category>ethics</category><category>massive_teaching</category><category>mooc</category><category>privacy</category><guid>http://paulolivier.dehaye.org/posts/extended-statement-on-massiveteaching-part-ii.html</guid><pubDate>Mon, 20 Oct 2014 22:00:37 GMT</pubDate></item><item><title>Statement on #massiveteaching (part I)</title><link>http://paulolivier.dehaye.org/posts/short-statement-on-massiveteaching-part-i.html</link><dc:creator>Paul-Olivier Dehaye</dc:creator><description>&lt;p&gt;(For a more extended statement, see &lt;a class="reference external" href="http://paulolivier.dehaye.org/posts/extended-statement-on-massiveteaching-part-ii.html"&gt;here&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;My name is Paul-Olivier Dehaye, I am a mathematics professor at the University of Zurich. On June 23rd 2014, I started to teach a MOOC on Coursera, called &lt;em&gt;Teaching goes massive: New skills required&lt;/em&gt;, which was intended to last for three weeks and include a component on business practices of MOOC providers. During the course delivery, my perception of the ethical issues surrounding the course changed, which led me to alter its delivery methods. This prompted a reaction by Coursera itself, and I was censored &lt;a class="footnote-reference" href="http://paulolivier.dehaye.org/posts/short-statement-on-massiveteaching-part-i.html#id8" id="id1"&gt;[1]&lt;/a&gt;. In the fallout, I was also insulted and vilified online. Coursera used intentionally misleading information and deeply intrusive data held about the course and me to try to get my employer to launch a disciplinary procedure against me. Legal threats were also made.&lt;/p&gt;
&lt;p&gt;Throughout my course, I acted with the interest of the European and Swiss public in mind. The ethical concerns that led me to change the course so abruptly hinged on strong similarities between the Coursera and Facebook Terms of Use, and the lack of transparency and accountability on the data collection and "research" practices at those companies.&lt;/p&gt;
&lt;p&gt;There seems to be a very different perception across the Atlantic on privacy. As higher education moves online globally, it is important that the rest of the world does not adopt by default a Silicon Valley narrative on privacy issues in the educational domain. In my view, it is of primordial importance that European universities do not contribute to an erosion of the privacy values held by their local taxpayers &lt;a class="footnote-reference" href="http://paulolivier.dehaye.org/posts/short-statement-on-massiveteaching-part-i.html#id9" id="id2"&gt;[2]&lt;/a&gt;. There are fundamentally different regulatory frameworks across the Atlantic: while privacy is seen in Europe as a human rights issue, it tends to be seen in the US as a tradable commodity, akin to a property right. If we consider instead developing countries, we are currently at a juncture point on this issue: while it is no doubt beneficial to offer access to educational material for their next generation, it is questionable whether this requires exporting business practices that are under increasingly intense scrutiny at home.&lt;/p&gt;
&lt;p&gt;Privacy is only one of my concerns associated to Coursera and the current shaping of the MOOC market.&lt;/p&gt;
&lt;p&gt;By muddling the thick legitimacy of our universities with the thin legitimacy of companies backed by venture capital, we risk devaluing our core: academic freedom. This dynamic happened in other industries, such as journalism, where it has led to a complete reversal of power structures, and a struggle of journalists to remain relevant &lt;a class="footnote-reference" href="http://paulolivier.dehaye.org/posts/short-statement-on-massiveteaching-part-i.html#id10" id="id3"&gt;[3]&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Beyond delivering educational materials worldwide, I am optimistic for the potential of MOOCs to create rich and meaningful experiences for students, through citizen science and open democracy activities for instance. There is a rapid impulse to apply similar combinations of educational and crowdsourcing techniques at a high cognitive level to the labor market &lt;a class="footnote-reference" href="http://paulolivier.dehaye.org/posts/short-statement-on-massiveteaching-part-i.html#id11" id="id4"&gt;[4]&lt;/a&gt;. The utopia is that this can solve labor issues on a mass scale, enabling students to extract value from their degrees right away. More ambiguously though, through &lt;em&gt;turking&lt;/em&gt; &lt;a class="footnote-reference" href="http://paulolivier.dehaye.org/posts/short-statement-on-massiveteaching-part-i.html#id12" id="id5"&gt;[5]&lt;/a&gt; and the privatisation of higher education certification on a large scale, this perniciously opens the door to seamless integration of these two markets into a more exploitative arrangement.&lt;/p&gt;
&lt;p&gt;Finally, we should not forget what we lose by automating any component of an educational experience. Algorithmic culture comes with its own problems (especially when those algorithms are opaque &lt;a class="footnote-reference" href="http://paulolivier.dehaye.org/posts/short-statement-on-massiveteaching-part-i.html#id13" id="id6"&gt;[6]&lt;/a&gt;), and big data is by definition discriminatory &lt;a class="footnote-reference" href="http://paulolivier.dehaye.org/posts/short-statement-on-massiveteaching-part-i.html#id14" id="id7"&gt;[7]&lt;/a&gt; and hence a threat to any attempt at fostering equality in education.&lt;/p&gt;
&lt;table class="docutils footnote" frame="void" id="id8" rules="none"&gt;&lt;colgroup&gt;&lt;col class="label"&gt;&lt;col&gt;&lt;/colgroup&gt;&lt;tbody valign="top"&gt;&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="http://paulolivier.dehaye.org/posts/short-statement-on-massiveteaching-part-i.html#id1"&gt;[1]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;My profile is missing &lt;a class="reference external" href="https://www.coursera.org/zurich"&gt;from here&lt;/a&gt;, for instance.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table class="docutils footnote" frame="void" id="id9" rules="none"&gt;&lt;colgroup&gt;&lt;col class="label"&gt;&lt;col&gt;&lt;/colgroup&gt;&lt;tbody valign="top"&gt;&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="http://paulolivier.dehaye.org/posts/short-statement-on-massiveteaching-part-i.html#id2"&gt;[2]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;As an example, Coursera &lt;a class="reference external" href="http://safeharbor.export.gov/companyinfo.aspx?id=21417"&gt;refuses to comply with the EU and/or Swiss Data Protection Authorities&lt;/a&gt;.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table class="docutils footnote" frame="void" id="id10" rules="none"&gt;&lt;colgroup&gt;&lt;col class="label"&gt;&lt;col&gt;&lt;/colgroup&gt;&lt;tbody valign="top"&gt;&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="http://paulolivier.dehaye.org/posts/short-statement-on-massiveteaching-part-i.html#id3"&gt;[3]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;As masterfully explained by Jay Rosen, journalism professor at NYU, in &lt;a class="reference external" href="http://www.theatlantic.com/technology/archive/2014/07/facebook-has-all-the-power-you-have-almost-none/374215/"&gt;two&lt;/a&gt; &lt;a class="reference external" href="http://www.washingtonpost.com/posteverything/wp/2014/07/03/dont-blame-facebook-for-screwing-with-your-mood-blame-academia/"&gt;articles&lt;/a&gt; written in the wake of the Facebook Emotion experiment. I detail some of &lt;a class="reference external" href="http://paulolivier.dehaye.org/posts/erosion-of-thick-legitimacy-by-coursera.html"&gt;my concerns here&lt;/a&gt;.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table class="docutils footnote" frame="void" id="id11" rules="none"&gt;&lt;colgroup&gt;&lt;col class="label"&gt;&lt;col&gt;&lt;/colgroup&gt;&lt;tbody valign="top"&gt;&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="http://paulolivier.dehaye.org/posts/short-statement-on-massiveteaching-part-i.html#id4"&gt;[4]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;See the two workshops at &lt;a class="reference external" href="http://www.humancomputation.com/2014/"&gt;HCOMP 2014&lt;/a&gt; for these trends in &lt;em&gt;human computation&lt;/em&gt;.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table class="docutils footnote" frame="void" id="id12" rules="none"&gt;&lt;colgroup&gt;&lt;col class="label"&gt;&lt;col&gt;&lt;/colgroup&gt;&lt;tbody valign="top"&gt;&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="http://paulolivier.dehaye.org/posts/short-statement-on-massiveteaching-part-i.html#id5"&gt;[5]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;&lt;a class="reference external" href="http://en.wikipedia.org/wiki/Amazon_Mechanical_Turk"&gt;"A practice that enables individuals or businesses to coordinate the use of human intelligence to perform tasks that computers are currently unable to do."&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table class="docutils footnote" frame="void" id="id13" rules="none"&gt;&lt;colgroup&gt;&lt;col class="label"&gt;&lt;col&gt;&lt;/colgroup&gt;&lt;tbody valign="top"&gt;&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="http://paulolivier.dehaye.org/posts/short-statement-on-massiveteaching-part-i.html#id6"&gt;[6]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;See &lt;a class="reference external" href="http://socialmediacollective.org/2014/03/25/show-and-tell-algorithmic-culture/"&gt;Show-and-Tell: Algorithmic Culture&lt;/a&gt; and &lt;a class="reference external" href="http://socialmediacollective.org/2014/06/26/corrupt-personalization/"&gt;Corrupt Personalisation&lt;/a&gt;.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table class="docutils footnote" frame="void" id="id14" rules="none"&gt;&lt;colgroup&gt;&lt;col class="label"&gt;&lt;col&gt;&lt;/colgroup&gt;&lt;tbody valign="top"&gt;&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="http://paulolivier.dehaye.org/posts/short-statement-on-massiveteaching-part-i.html#id7"&gt;[7]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;See &lt;a class="reference external" href="http://papers.ssrn.com/sol3/papers.cfm?abstract_id=2477899"&gt;Big Data's Disparate Impact&lt;/a&gt;.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;</description><category>connected_course</category><category>coursera</category><category>ethics</category><category>massive_teaching</category><category>mooc</category><category>privacy</category><category>scholar14</category><category>t509massive</category><guid>http://paulolivier.dehaye.org/posts/short-statement-on-massiveteaching-part-i.html</guid><pubDate>Mon, 20 Oct 2014 21:43:30 GMT</pubDate></item><item><title>Erosion of thick legitimacy by Coursera</title><link>http://paulolivier.dehaye.org/posts/erosion-of-thick-legitimacy-by-coursera.html</link><dc:creator>Paul-Olivier Dehaye</dc:creator><description>&lt;p&gt;In my &lt;a class="reference external" href="http://paulolivier.dehaye.org/posts/thin-legitimacy-at-whisper-facebook-and-coursera.html"&gt;previous post&lt;/a&gt;, I raised the possibility that the thick legitimacy of academics would be eroded by meddling it with the thin legitimacy of Coursera (or other MOOC providers). This echoes a useful distinction that was introduced by Jay Rosen, in the wake of the Facebook Emotions experiment.&lt;/p&gt;
&lt;p&gt;Is this a theoretical risk? No, unfortunately. I can offer concrete cases tied to my Coursera MOOC &lt;em&gt;Massive Teaching: New skills required&lt;/em&gt;. These should explain why that course went South.&lt;/p&gt;
&lt;p&gt;What happened in my course is certainly complex, and this not meant to be an account of it. It discusses my reasoning about ethics, and focuses on the complexity of that interaction between thick and thin legitimacies. It ignores much of what happened when interacting with the students themselves. I finished my previous post with a video from my course outlining how different the startup and scientific processes are.&lt;/p&gt;
&lt;br&gt;&lt;center&gt;
&lt;iframe width="560" height="315" src="//www.youtube.com/embed/3SI7-oDqoFI" frameborder="0" allowfullscreen&gt;&lt;/iframe&gt;
&lt;/center&gt;
&lt;br&gt;&lt;p&gt;For my own teaching, I had adopted a model that I will call &lt;em&gt;Agile Teaching&lt;/em&gt;. In fact, I explained what I meant by this in a separate video for the course:&lt;/p&gt;
&lt;br&gt;&lt;center&gt;
&lt;iframe width="560" height="315" src="//www.youtube.com/embed/1xnBH0JDaU8?list=PLtqHSxfnLCmA_E4gk-tmklKoXfEoXSly9" frameborder="0" allowfullscreen&gt;&lt;/iframe&gt;
&lt;/center&gt;
&lt;br&gt;&lt;p&gt;You can see several concepts there that reflect startup culture and the Agile methodology: failure and risks are OK, rapid iteraton, responsivity. This is similar to what Derek Bruff calls &lt;em&gt;Agile Teaching&lt;/em&gt; (&lt;a class="reference external" href="http://derekbruff.org/?page_id=1512"&gt;I think he originated the term&lt;/a&gt;). In my case, I included delivery and production as part of the concept. This actually worked very well within my course, and the technique can certainly be applied to many topics.&lt;/p&gt;
&lt;p&gt;However, since the topic was partly the business model of MOOC providers, and I was teaching so responsively, this technique is bound to hit some snags. During the preparation of the course, many interactions with the Coursera staff made me uncomfortable with their willingness to be transparent. This is bad practice but understandable, as I knew I was pushing them too fast and they were not used to that. I found other solutions, which included the a-posteriori-analysis-of-already-collected-data trick described in my previous post, for &lt;em&gt;teaching purposes&lt;/em&gt;. When, &lt;strong&gt;during the course&lt;/strong&gt;, the Facebook experiment makes the news, I went to re-read the section on education research in the Coursera Terms of Use.&lt;/p&gt;
&lt;blockquote class="epigraph"&gt;
&lt;p&gt;Records of your participation in Online Courses may be used for researching online education. In the interests of this research, you may be exposed to slight variations in the course materials that will not substantially alter your learning experience. All research findings will be reported at the aggregate level and will not expose your personal identity.&lt;/p&gt;
&lt;p class="attribution"&gt;—&lt;em&gt;Online Education Research&lt;/em&gt; in &lt;a class="reference external" href="https://www.coursera.org/about/terms"&gt;Coursera Terms of Use&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The abrupt change in the course comes from my initial misunderstanding of this passage. The first time I read it, months before the course started, I understood that it referred to research performed by my peers, with thick legitimacy. But the Facebook experiment made it clear to me that I was mistaken: this is also and primarily used to justify research/product improvement with thin legitimacy, which can be very thin indeed. Coursera is also more dangerous in that sense than Facebook, since it actively attempts to clout itself with thick legitimacy and blend the two. To give a few examples:&lt;/p&gt;
&lt;ul class="simple"&gt;&lt;li&gt;there are university administrators sitting on its &lt;a class="reference external" href="https://www.coursera.org/about/leadership"&gt;University Advisory Board&lt;/a&gt;, but that power is, as far as I know, untested;&lt;/li&gt;
&lt;li&gt;they rely on AAUP membership to select US partners;&lt;/li&gt;
&lt;li&gt;they rely on Shangai rankings to select non-US partners.&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;Between June 29th and July 3rd, I followed the same train of thought as Jay Rosen when writing his July 3rd Washington Post article, and ultimately see three ethical dangers tied to this blending of thick and thin legitimacies:&lt;/p&gt;
&lt;ul class="simple"&gt;&lt;li&gt;of abuses by academics, through the a-posteriori-analysis-of-already-collected-data trick (was what I myself did in the first week ethical? What about what I was going to do in the second week?)&lt;/li&gt;
&lt;li&gt;of abuses against the learners by Coursera, just as with the Facebook experiment (do A/B tests of teaching interventions, robots on Coursera forums and a non-transparent list of other experiments require internal ethical approval? Where is Coursera going with the right to replay student comments at will?);&lt;/li&gt;
&lt;li&gt;of abuses against professsors or their profession by Coursera (by devaluing the thicker legitimacy, the &lt;a class="reference external" href="http://paulolivier.dehaye.org/posts/moocs-journalism-and-digital-disruption.html"&gt;parallel with journalism and photography&lt;/a&gt; is strong).&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;I resolved that ultimately, in those conditions, I could not compromise my own thick legitimacy with the thinner legitimacy of Coursera. This was probably exacerbated by the topic of my course: by encouraging discussion about MOOCs in the course, and encouraging sharing of information about wishes and wants of students in the domain of MOOCs, I was in fact actively contributing to this blending of thick and thin legitimacies.&lt;/p&gt;
&lt;p&gt;My response was to pull content from the course, while not abandonning my students (by still trying to explain this on the forums, within legal limits). This action prompted a reaction from Coursera, which was to give me a deadline to reinstate the content. Once given that deadline, I attempted to raise ethical concerns within my own university (address the issue at the level of thick legitimacy) to resolve the situation. Unfortunately, through different channels the thinner legitimacy prevailed, Coursera did not respect its own deadline and I was prevented from explaining myself with my students. Ironically, pretty much at the exact same time, Rosen was putting up this on the Washington Post site:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;When the study’s methods became controversial in the public square, in the press, and in online conversation, that should be a moment for the university to shine. Our strengths include: Academic freedom. Knowing what you’re talking about. “Yes, we thought of that.” To the press or to anyone else who has questions about it, we should be happy to explain our research, including ethical issues as they arise. As academics, we pride ourselves on thinking these things through. And we have procedures! If you experiment on human beings you have to follow them. Academic research is not some free-for-all. It has to meet certain standards. When those standards become controversial in the public square we are happy to explain them. Because we know what we’re doing—&lt;/p&gt;
&lt;p&gt;Except when we don’t. Reached by the Atlantic magazine, one of the academics researchers on the Facebook study chose silence rather than “let me explain our research design.”&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I didn't choose silence, and this was not &lt;em&gt;research&lt;/em&gt; with thick legitimacy. This was teaching with thick legitimacy. But Coursera itself is doing research with thin legitimacy, now or in the future, that I could not allow without more guarantees, that they had already refused four or five days before the Facebook experiment made the news. Possibly that stance is exaggerated, but I would welcome a discussion on this any time.&lt;/p&gt;
&lt;p&gt;In any case, when the &lt;a class="reference external" href="http://idstuff.blogspot.ch/2014/07/social-experiment-learning-experience.html"&gt;first blog post about the course came out&lt;/a&gt;, this forced silence (and PR statements issued by the thin legitimacy outfit) led to suspicion that I was myself performing experiments, violating my own thick legitimacy by not following IRB protocol. The result? The author suggests the following in the last paragraph:&lt;/p&gt;
&lt;blockquote&gt;
From a research point of view this is FASCINATING.  I would love to get a hold of the discussion forum data for both discourse and corpus linguistics analyses. On the other hand, I fear that coursera, and all involved parties, are handling this one wrong again.  We are now entering the third and final week of this MOOC on MOOCs.  Let's see how this pans out.&lt;/blockquote&gt;
&lt;p&gt;In other words, the author simultaneously complains about a situation, but wishes he could put his hands on the dataset so he could do research with thick legitimacy. He will of course never have access to that data, since Coursera owns it, from day one. Actually, there are many things that are not very inspiring about the technical architecture of the platform itself, so it might very well be that once an instructor deletes a forum, its content is unrecoverable. That's why I deleted forums when I could: to protect students, despite not having the opportunity to explain exactly why I did it (because of legal risk). Nothing has shown to me since that Coursera was able to retrieve that content.&lt;/p&gt;
&lt;p&gt;The third week of the course was supposed to contain a final, peer-feedback quiz. That quiz would be asking students generic questions about MOOCs and what they learned in the course. The same author talks about that assignment &lt;a class="reference external" href="http://idstuff.blogspot.ch/2014/07/youve-been-punkd-however-that-was.html"&gt;here&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
With regard to this "Assignment" I feel rather cynical on all fronts. On the one hand it feels like this is just another data-gathering stunt. So Paul ran his "experiment" and now he is collecting data to see what the learners say. The learners that didn't un-enroll from the course that is.  On the other hand, even if this is an earnest attempt to have learners introspect on this whole process, the attempt falls really flat on its face because this data is tainted. The questions don't address anything that happened in the course. It feels like these were written with the original learning objectives in mind, and as such it reduces this final exercise into a farce. It is a farce that does not respect the learners, and it is a farce of the educational process.&lt;/blockquote&gt;
&lt;p&gt;It's all of this, actually, and more. This was merely a draft, meant to validate the idea with Coursera staff of data-gathering in this way. But it was &lt;em&gt;also&lt;/em&gt; an earnest attempt to have learners introspect on this whole process, written with the original learning objectives in mind. I intended to tweak this draft later (maybe in a later iteration) into a more formal process, to be able to offer at scale personalised degrees that would reflect the learning that actually happened in the course. Once everything else happened, indeed the data &lt;em&gt;was&lt;/em&gt; tainted and it didn't address anything that happened in the course. The final exercise was reduced into a farce that did not respect the learners. By that time though, I was not involved anymore, and had objected multiple times to running the course and this assignment without me.&lt;/p&gt;
&lt;p&gt;After the assignment was completed by the students, that same author and some commenters on his blog &lt;a class="reference external" href="http://idstuff.blogspot.ch/2014/07/massiveteaching-experiment-falls-on.html"&gt;managed to hack into Coursera&lt;/a&gt; to get access to the data of all the students of the course. They were surprised at the diversity of responses, reflecting their own biases in understanding the complexity of the situation. On top, their interst in this data validated my own concerns: this data, beyond being useful for teaching, was valuable to research on the learner experience yet would be collected without IRB approval (it was merely used for teaching). My own thick legitimacy would enable its collection, yet I would have no option available to protect it from the eyes of Coursera, whose legitimacy is different and who might have genuine interest in that data.&lt;/p&gt;</description><category>coursera</category><category>ethics</category><category>massiveteaching</category><guid>http://paulolivier.dehaye.org/posts/erosion-of-thick-legitimacy-by-coursera.html</guid><pubDate>Mon, 20 Oct 2014 13:21:14 GMT</pubDate></item><item><title>Learning, working and ?</title><link>http://paulolivier.dehaye.org/posts/learning-working-and.html</link><dc:creator>Paul-Olivier Dehaye</dc:creator><description>&lt;p&gt;A lot of my recent thoughts have turned around issues of crowdsourcing and online education.&lt;/p&gt;
&lt;img alt="../worklearn.jpg" class="align-right" src="http://paulolivier.dehaye.org/worklearn.jpg" style="width: 346.4px; height: 85.6px;"&gt;&lt;p&gt;One of my co-authors (&lt;a class="reference external" href="http://hci.uni-hannover.de/people/markus"&gt;Markus Krause&lt;/a&gt;) is co-organising a workshop on that topic, &lt;a class="reference external" href="http://www.worklearn.org/"&gt;WorkLearn 2014&lt;/a&gt;. It will take place in Pittsburgh November 2-4, as part of the &lt;a class="reference external" href="http://www.humancomputation.com/2014/"&gt;Human Computation conference HCOMP2014&lt;/a&gt;. Human Computation is another expression interchangeable for &lt;a class="reference external" href="http://paulolivier.dehaye.org/posts/social-teaching-machines.html"&gt;social machine&lt;/a&gt;, although it has different connotations.&lt;/p&gt;
&lt;p&gt;The stated motivation of the workshop is very ambitious:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The online education and crowdsourcing communities are addressing similar problems in educating, motivating and evaluating students and workers. The online learning community succeeds in increasing the supply side of the cognitively skilled labor market, and the crowdsourcing at scale community creates a larger marketplace for cognitively skilled work.&lt;/p&gt;
&lt;p&gt;Linking online platforms for crowd work with platforms for MOOCs has the potential to: provide knowledge and training at a massive scale to contributors; collect data that identify expert skills; engage contributors in simultaneously working and learning in a social environment; and organize large communities around online courses on specific topics. These all provide new opportunities to support and deploy sophisticated algorithms for crowd learning and work.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The most successful example in this direction is of course &lt;a class="reference external" href="http://duolingo.com"&gt;Duolingo&lt;/a&gt;, which helps translate the web while using volunteer labor by language learners. If one omits the learning, the strategy there is not that different from the strategy used by my Coursera coworker &lt;a class="reference external" href="https://www.coursera.org/instructor/bernstein"&gt;Abraham Bernstein&lt;/a&gt; to translate books using &lt;a class="reference external" href="http://en.wikipedia.org/wiki/Amazon_Mechanical_Turk"&gt;Amazon Mechanical Turk&lt;/a&gt;, and indeed part of his efforts aim to design effective tools to program those social machines (with the programming language CrowdLang).&lt;/p&gt;
&lt;embed&gt;&lt;iframe width="640" height="360" src="//www.youtube.com/embed/emCABRV2cUA" frameborder="0" allowfullscreen&gt;&lt;/iframe&gt;
&lt;/embed&gt;&lt;p&gt;I have always had some qualms about the &lt;a class="reference external" href="http://florianschmidt.co/the-good-the-bad-and-the-ugly/"&gt;ethics of crowdsourcing&lt;/a&gt;, even though it can clearly be used for good: the prototypical success story is in the work of Ushahidi during the &lt;a class="reference external" href="http://www.ushahidi.com/blog/2012/01/12/haiti-and-the-power-of-crowdsourcing/"&gt;2010 Haiti earthquake&lt;/a&gt;. I was thus very happy to see over Labor Day 2014 that Michael Bernstein from Stanford announced &lt;a class="reference external" href="http://crowdresearch.org/blog/?p=9039"&gt;guidelines for academic requesters on Amazon Mechanical Turk&lt;/a&gt;. He explains the rationale for this (a Turker is a worker on a crowdsourcing platform):&lt;/p&gt;
&lt;blockquote&gt;
An IRB-approved researcher experimented on the [crowdsourcing] platform unannounced. The result was Turker confusion, strife, and wasted time, in a system where time is what it takes to make ends meet.&lt;/blockquote&gt;
&lt;p&gt;These guidelines were themselves crowdsourced, designed together with the Turkers (it's only natural!).&lt;/p&gt;
&lt;p&gt;At the same time, over the summer, there was a huge controversy over the iffy ethics of social platforms experimentation. This is due to the release at the very end of June 2014 of a &lt;a class="reference external" href="http://www.forbes.com/sites/kashmirhill/2014/06/29/facebook-doesnt-understand-the-fuss-about-its-emotion-manipulation-study/"&gt;Facebook experiment on its users&lt;/a&gt; (don't miss the Cornell IRB flowchart there). There are a ton of links about this, but the best is probably the account by Mary L. Gray of an &lt;a class="reference external" href="http://marylgray.org/?page_id=203"&gt;ethics panel that took place at the Microsoft Research Faculty Summit&lt;/a&gt; (and unfortunately was published with much delay).&lt;/p&gt;
&lt;p&gt;In any case, this should give serious pause to any educator. One can see lots of fields suddenly getting much too close, with very different or inexistent values. Online learners, just as Turkers, are vulnerable. &lt;a class="reference external" href="http://nogoodreason.typepad.co.uk/no_good_reason/2014/06/the-ethics-of-digital-scholarship.html"&gt;Martin Weller&lt;/a&gt; and &lt;a class="reference external" href="http://www.elearnspace.org/blog/2014/01/13/the-vulnerability-of-learning/"&gt;George Siemens&lt;/a&gt; have recently insisted on this.&lt;/p&gt;
&lt;p&gt;So, what do you think? Anyone wants to submit a position paper (2 pages) on the topic? Any of my co-learners in MOOCs would like to see what we can do? We could, well... crowdsource it...&lt;/p&gt;
&lt;p&gt;(of course, this was due yesterday: official deadline is "September")&lt;/p&gt;</description><category>connected_course</category><category>coursera</category><category>crowdsourcing</category><category>duolingo</category><category>ethics</category><category>t509massive</category><category>whyopen</category><guid>http://paulolivier.dehaye.org/posts/learning-working-and.html</guid><pubDate>Thu, 11 Sep 2014 00:17:10 GMT</pubDate></item><item><title>"Don't be evil", or how I learned to behave like a startup and love the data</title><link>http://paulolivier.dehaye.org/posts/dont-be-evil-or-how-i-learned-to-behave-like-a-startup-and-love-the-data.html</link><dc:creator>Paul-Olivier Dehaye</dc:creator><description>&lt;img alt="../strangelove.png" class="align-right" src="http://paulolivier.dehaye.org/strangelove.png" style="width: 320.0px; height: 240.0px;"&gt;&lt;p&gt;When Gmail was opened in 2004, I received invitations early. If I remember well, they came from a friend working at Google who had already snatched a few fun login names. I did the same, and passed on further invitations to my brother and our friends back home.&lt;/p&gt;
&lt;p&gt;A year or so later, when my brother was visiting with his friends, we went on a tour of the Googleplex. Randomly passing in front of the cubicle of a homonym, one of the friends suddenly realised why he had not been able to register his own name earlier. In other words, &lt;strong&gt;an unknown collision in the physical world had first manifested digitally&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;I like to think of those collisions as the digital equivalent of New York overcrowding, trying to fit too many people in just a few login characters.&lt;/p&gt;
&lt;p&gt;So which fun pseudonyms did we chose? Which did we consider worthy in this land grab? Certainly many of them were aimed at our shared cultural backgrounds as Belgians in the Silicon Valley. If you had &lt;a class="reference external" href="mailto:tintin@gmail.com"&gt;tintin@gmail.com&lt;/a&gt;, or &lt;a class="reference external" href="http://nl.wikipedia.org/wiki/Frietkot"&gt;frietkot@gmail.com&lt;/a&gt; that would be pretty impressive, no? Indeed, we grabbed names of regions, superheroes, movie stars, concepts, etc. We certainly thought this was OK, and didn't reflect more on something that &lt;a class="reference external" href="http://en.wikipedia.org/wiki/Nymwars"&gt;became controversial only later&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;One of the logins I grabbed had the name of a Belgian politician, let's call him Some Guy. He was on TV and I thought my friends would get a chuckle if I emailed them from it. Certainly, I might have crossed a moral line already then, but it felt like a very tiny escalation in this virtual land grab.&lt;/p&gt;
&lt;p&gt;What did I do with this account? I mostly used it for spam protection. I set it up so that all emails sent there would be forwarded to my default inbox, and gave this address whenever there was a need to a register for a spammy online service. This worked well, possibly because Gmail's algorithms had learned to weigh emails transiting through this address differently and benefited from the additional segmenting.&lt;/p&gt;
&lt;p&gt;Around 2008, inevitably, I started receiving emails addressed to That Guy. Those collisions happen to all of us, for all of our email accounts. What is the moral thing to do there? My philosophy is most of the time to let it drop, but  sometimes also to reply to the sender telling them that they got the wrong address (due to emails missent to my main account, I must have had to contact a dozen hotels in Quebec by now). In most cases, the only way to know what to do is to read the email, slightly invading this other persons' privacy.&lt;/p&gt;
&lt;blockquote&gt;
Just like Rachel and her Friends in their New York apartment, we struggle to deal with those privacy collisions, especially when we feel a need to intervene.&lt;/blockquote&gt;
&lt;iframe width="900" height="600" src="//www.youtube.com/embed/tYn8s0_kDUw?rel=0&amp;amp;hd=1&amp;amp;wmode=transparent"&gt;&lt;/iframe&gt;&lt;p&gt;For That Guy, it was even easier to feel morally OK about it: I never actively sought the emails, had no way to prevent the mistake, and anyways the emails were from cranks. On top, by that time I had registered to too many services with that pseudonym, which effectively tied my identity to it, with no way to revert the situation. So in effect this data collection was happening, whether I liked it or not, or at least that was my moral justification.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The problem with data is that it leaks.&lt;/strong&gt; The cranks don't just email one influential person at a time. They email a few, who are susceptible to know each other. As a consequence, in this case, the cranks polluted those recipients' email software  with a wrong email address. Of course, in due time, the email autocompletion software of those recipients started tripping them and I started receiving emails from other politicians to That Guy. &lt;strong&gt;Algorithmic curation had gone wrong, and actively mislead humans.&lt;/strong&gt; The fact that these were politicians might have mislead me: I should have made the effort of explaining the awkward situation to That Guy's interlocutors and tried to correct it. But I didn't. Somehow a couple more emails made it to me that were clearly of more social nature. Again, I didn't do anything. &lt;strong&gt;This data will not disappear unless actively deleted&lt;/strong&gt;, and even then I can only be so sure.&lt;/p&gt;
&lt;blockquote&gt;
At this point you will deservedly think that I am a moron. But was it morally wrong? And when exactly did it go wrong?&lt;/blockquote&gt;
&lt;p&gt;Throughout my moral justification was that I was not actively seeking this. Emails would land in my mailbox and I would have to read them to know what to do. Of course, this conveniently ignores what I could have done to prevent those emails to arrive in the first place. Part of my justification was that I wasn't doing anything with the data collected. There was no clear goal, except &lt;strong&gt;awareness that this could be used to make a point later, which I guess I am making here now publicly&lt;/strong&gt; (in fact, I have used this to make this point in private throughout the years).&lt;/p&gt;
&lt;p&gt;The more interesting issue here is to understand that this is exactly how many big data companies function. "Don't be evil" Google gobbles data all over the place &lt;a class="reference external" href="http://www.wired.com/2014/04/threatlevel_0401_streetview/"&gt;for purposes that are not always clear at the time&lt;/a&gt;, and the justification is often that this was incidental, automated and did not require human intervention. Looking at a corporate setting elevates the stakes, and my feeble moral justifications are not sufficient anymore. It becomes a matter of ethics, which arguably should be that data collection is by default unethical: data should not be kept beyond the time necessary for its intended use, with that use itself subject to precise and established ethical rules. It looks like Google has understood this in some markets, for instance education (unlike other players there), and this will be the topic of a later post.&lt;/p&gt;
&lt;p&gt;(Image in the public domain: the Dr Stangelove War Room, which &lt;a class="reference external" href="http://valleywag.gawker.com/airbnbs-office-has-a-replica-of-the-dr-strangelove-wa-1475788543"&gt;happens to be replicated in the Airbnb HQ&lt;/a&gt;)&lt;/p&gt;</description><category>ethics</category><category>privacy</category><guid>http://paulolivier.dehaye.org/posts/dont-be-evil-or-how-i-learned-to-behave-like-a-startup-and-love-the-data.html</guid><pubDate>Mon, 08 Sep 2014 09:45:51 GMT</pubDate></item><item><title>Keeping a Soul in the Driver's Seat</title><link>http://paulolivier.dehaye.org/posts/keeping-a-soul-in-the-driver-seat.html</link><dc:creator>Paul-Olivier Dehaye</dc:creator><description>&lt;img alt="../railway.jpg" class="align-right" src="http://paulolivier.dehaye.org/railway.jpg"&gt;&lt;p&gt;I can't wait for driverless cars. Ten years is the estimate. Combined with car sharing, they will revolutionise our cities and make them much more efficient and livable. They should bring about umitigated good to our society. Yet they still come with ethical challenges, usually categorized as &lt;a class="reference external" href="http://en.wikipedia.org/wiki/Trolley_problem"&gt;trolley problems&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Wired just published an article called &lt;a class="reference external" href="http://www.wired.com/2014/08/heres-a-terrible-idea-robot-cars-with-adjustable-ethics-settings/"&gt;Here's a Terrible Idea: Robot Cars With Adjustable Ethics Settings&lt;/a&gt;, outlining the ethical issues involved in substituting human drivers with robots.&lt;/p&gt;
&lt;blockquote&gt;
In freak accidents, computers would have to take decisions such as &lt;strong&gt;killing one motorcyclist without a helmet vs. killing five pedestrians&lt;/strong&gt;.&lt;/blockquote&gt;
&lt;p&gt;The writer raises many such dystopian choices: &lt;strong&gt;children vs elderly&lt;/strong&gt;, &lt;strong&gt;us vs others&lt;/strong&gt;, &lt;strong&gt;rich vs poor&lt;/strong&gt;, etc. He rightfully sees a liability for anyone having to program those decisions. In his opinion, any attempt by the car manufacturer to distantiate itself from lawsuits by offering variable ethical settings to the owner of the car would not decrease the liability of the car manufacturer, and therefore this remains an obstacle to rolling in a driverless car.&lt;/p&gt;
&lt;blockquote&gt;
The car company has another option, which is missed by the writer: absorb progressively the insurance business.&lt;/blockquote&gt;
&lt;p&gt;First off, it's clear that any level of indirection and legal tangling is helpful in freak legal confrontations to shield car manufacturer from legal responsibility towards private individuals. Secondly, the writer does not give enough credit to the creativity of engineers/lawyers/business types.&lt;/p&gt;
&lt;blockquote&gt;
Why wouldn't they be able to introduce one further level of indirection? The manufacturer could build a "car without a soul".&lt;/blockquote&gt;
&lt;p&gt;The car could offer full access proprietary APIs to its raw or slightly processed data, but require linking to an ethical core library before it would start. This ethical core would only be called upon if a future collision is detected, and asked to respond to the really tough questions (or it could be run on a loop validating any driving input). Who would take the liability of writing such a core? Insurance companies would seem like the natural candidate. In fact, this is a very natural extension of their business, litigating for the choices they have coldly programmed in rather than the mistakes made by their clients. It would also make sense to decentralise geographically this ethical core, since driving customs are bound to vary from country to country (think of these &lt;a class="reference external" href="https://www.youtube.com/watch?v=7vd_OuqUAaI"&gt;comparatively safe Indian drivers&lt;/a&gt; or this &lt;a class="reference external" href="https://www.youtube.com/watch?v=RjrEQaG5jPM"&gt;Russian ninja&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;The question is whether insurance companies would be willing to go along. They would certainly feel pressure to adapt to a world of driverless cars, but the brilliant move for the car company would be to promise increased efficiency and reach to the whole insurance industry (more clients), and act as a middleman. By encouraging collaboration between the insurance companies, ostensibly to help them save money on R&amp;amp;D, standardise good practice, exchange regulatory tips, etc, the car company would crowdsource the insurance industry to force itself into obsolescence. This would allow the car company to eventually provide the full product, once all the R&amp;amp;D costs of the fine tuning of the ethical core have been shouldered by the insurance companies. Note that this core would only be ethical in name, as it would have been exclusively fine tuned with cost efficiency in mind.&lt;/p&gt;
&lt;p&gt;This assumes there is a car company that is sufficiently dominating the car industry to strong arm insurance companies.&lt;/p&gt;
&lt;p&gt;(For other futuristic and "fun" questions on the transformation brought about by driverless cars, see the amazingly cold-blooded &lt;a class="reference external" href="http://fortune.com/2014/08/15/if-driverless-cars-save-lives-where-will-we-get-organs/"&gt;If driverless cars save lives, where will we get organs?&lt;/a&gt;.)&lt;/p&gt;
&lt;p&gt;(Image credit: Wikipedia)&lt;/p&gt;</description><category>crowdsourcing</category><category>ethics</category><guid>http://paulolivier.dehaye.org/posts/keeping-a-soul-in-the-driver-seat.html</guid><pubDate>Fri, 22 Aug 2014 09:16:37 GMT</pubDate></item></channel></rss>