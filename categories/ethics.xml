<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>paulolivier.dehaye.org (ethics)</title><link>http://paulolivier.dehaye.org/</link><description></description><language>en</language><lastBuildDate>Mon, 20 Oct 2014 17:18:42 GMT</lastBuildDate><generator>http://getnikola.com/</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Erosion of thick legitimacy by Coursera</title><link>http://paulolivier.dehaye.org/posts/erosion-of-thick-legitimacy-by-coursera.html</link><dc:creator>Paul-Olivier Dehaye</dc:creator><description>&lt;p&gt;In my &lt;a class="reference external" href="http://paulolivier.dehaye.org/posts/thin-legitimacy-at-whisper-facebook-and-coursera.html"&gt;previous post&lt;/a&gt;, I raised the possibility that the thick legitimacy of academics would be eroded by meddling it with the thin legitimacy of Coursera (or other MOOC providers). This echoes a useful distinction that was introduced by Jay Rosen, in the wake of the Facebook Emotions experiment.&lt;/p&gt;
&lt;p&gt;Is this a theoretical risk? No, unfortunately. I can offer concrete cases tied to my Coursera MOOC &lt;em&gt;Massive Teaching: New skills required&lt;/em&gt;. These should explain why that course went South.&lt;/p&gt;
&lt;p&gt;What happened in my course is certainly complex, and this not meant to be an account of it. It discusses my reasoning about ethics, and focuses on the complexity of that interaction between thick and thin legitimacies. It ignores much of what happened when interacting with the students themselves. I finished my previous post with a video from my course outlining how different the startup and scientific processes are.&lt;/p&gt;
&lt;br&gt;&lt;center&gt;
&lt;iframe width="560" height="315" src="//www.youtube.com/embed/3SI7-oDqoFI" frameborder="0" allowfullscreen&gt;&lt;/iframe&gt;
&lt;/center&gt;
&lt;br&gt;&lt;p&gt;For my own teaching, I had adopted a model that I will call &lt;em&gt;Agile Teaching&lt;/em&gt;. In fact, I explained what I meant by this in a separate video for the course:&lt;/p&gt;
&lt;br&gt;&lt;center&gt;
&lt;iframe width="560" height="315" src="//www.youtube.com/embed/1xnBH0JDaU8?list=PLtqHSxfnLCmA_E4gk-tmklKoXfEoXSly9" frameborder="0" allowfullscreen&gt;&lt;/iframe&gt;
&lt;/center&gt;
&lt;br&gt;&lt;p&gt;You can see several concepts there that reflect startup culture and the Agile methodology: failure and risks are OK, rapid iteraton, responsivity. This is similar to what Derek Bruff calls &lt;em&gt;Agile Teaching&lt;/em&gt; (&lt;a class="reference external" href="http://derekbruff.org/?page_id=1512"&gt;I think he originated the term&lt;/a&gt;). In my case, I included delivery and production as part of the concept. This actually worked very well within my course, and the technique can certainly be applied to many topics.&lt;/p&gt;
&lt;p&gt;However, since the topic was partly the business model of MOOC providers, and I was teaching so responsively, this technique is bound to hit some snags. During the preparation of the course, many interactions with the Coursera staff made me uncomfortable with their willingness to be transparent. This is bad practice but understandable, as I knew I was pushing them too fast and they were not used to that. I found other solutions, which included the a-posteriori-analysis-of-already-collected-data trick described in my previous post, for &lt;em&gt;teaching purposes&lt;/em&gt;. When, &lt;strong&gt;during the course&lt;/strong&gt;, the Facebook experiment makes the news, I went to re-read the section on education research in the Coursera Terms of Use.&lt;/p&gt;
&lt;blockquote class="epigraph"&gt;
&lt;p&gt;Records of your participation in Online Courses may be used for researching online education. In the interests of this research, you may be exposed to slight variations in the course materials that will not substantially alter your learning experience. All research findings will be reported at the aggregate level and will not expose your personal identity.&lt;/p&gt;
&lt;p class="attribution"&gt;—&lt;em&gt;Online Education Research&lt;/em&gt; in &lt;a class="reference external" href="https://www.coursera.org/about/terms"&gt;Coursera Terms of Use&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The abrupt change in the course comes from my initial misunderstanding of this passage. The first time I read it, months before the course started, I understood that it referred to research performed by my peers, with thick legitimacy. But the Facebook experiment made it clear to me that I was mistaken: this is also and primarily used to justify research/product improvement with thin legitimacy, which can be very thin indeed. Coursera is also more dangerous in that sense than Facebook, since it actively attempts to clout itself with thick legitimacy and blend the two. To give a few examples:&lt;/p&gt;
&lt;ul class="simple"&gt;&lt;li&gt;there are university administrators sitting on its &lt;a class="reference external" href="https://www.coursera.org/about/leadership"&gt;University Advisory Board&lt;/a&gt;, but that power is, as far as I know, untested;&lt;/li&gt;
&lt;li&gt;they rely on AAUP membership to select US partners;&lt;/li&gt;
&lt;li&gt;they rely on Shangai rankings to select non-US partners.&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;Between June 29th and July 3rd, I followed the same train of thought as Jay Rosen when writing his July 3rd Washington Post article, and ultimately see three ethical dangers tied to this blending of thick and thin legitimacies:&lt;/p&gt;
&lt;ul class="simple"&gt;&lt;li&gt;of abuses by academics, through the a-posteriori-analysis-of-already-collected-data trick (was what I myself did in the first week ethical? What about what I was going to do in the second week?)&lt;/li&gt;
&lt;li&gt;of abuses against the learners by Coursera, just as with the Facebook experiment (do A/B tests of teaching interventions, robots on Coursera forums and a non-transparent list of other experiments require internal ethical approval? Where is Coursera going with the right to replay student comments at will?);&lt;/li&gt;
&lt;li&gt;of abuses against professsors or their profession by Coursera (by devaluing the thicker legitimacy, the &lt;a class="reference external" href="http://paulolivier.dehaye.org/posts/moocs-journalism-and-digital-disruption.html"&gt;parallel with journalism and photography&lt;/a&gt; is strong).&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;I resolved that ultimately, in those conditions, I could not compromise my own thick legitimacy with the thinner legitimacy of Coursera. This was probably exacerbated by the topic of my course: by encouraging discussion about MOOCs in the course, and encouraging sharing of information about wishes and wants of students in the domain of MOOCs, I was in fact actively contributing to this blending of thick and thin legitimacies.&lt;/p&gt;
&lt;p&gt;My response was to pull content from the course, while not abandonning my students (by still trying to explain this on the forums, within legal limits). This action prompted a reaction from Coursera, which was to give me a deadline to reinstate the content. Once given that deadline, I attempted to raise ethical concerns within my own university (address the issue at the level of thick legitimacy) to resolve the situation. Unfortunately, through different channels the thinner legitimacy prevailed, Coursera did not respect its own deadline and I was prevented from explaining myself with my students. Ironically, pretty much at the exact same time, Rosen was putting up this on the Washington Post site:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;When the study’s methods became controversial in the public square, in the press, and in online conversation, that should be a moment for the university to shine. Our strengths include: Academic freedom. Knowing what you’re talking about. “Yes, we thought of that.” To the press or to anyone else who has questions about it, we should be happy to explain our research, including ethical issues as they arise. As academics, we pride ourselves on thinking these things through. And we have procedures! If you experiment on human beings you have to follow them. Academic research is not some free-for-all. It has to meet certain standards. When those standards become controversial in the public square we are happy to explain them. Because we know what we’re doing—&lt;/p&gt;
&lt;p&gt;Except when we don’t. Reached by the Atlantic magazine, one of the academics researchers on the Facebook study chose silence rather than “let me explain our research design.”&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I didn't choose silence, and this was not &lt;em&gt;research&lt;/em&gt; with thick legitimacy. This was teaching with thick legitimacy. But Coursera itself is doing research with thin legitimacy, now or in the future, that I could not allow without more guarantees, that they had already refused four or five days before the Facebook experiment made the news. Possibly that stance is exaggerated, but I would welcome a discussion on this any time.&lt;/p&gt;
&lt;p&gt;In any case, when the &lt;a class="reference external" href="http://idstuff.blogspot.ch/2014/07/social-experiment-learning-experience.html"&gt;first blog post about the course came out&lt;/a&gt;, this forced silence (and PR statements issued by the thin legitimacy outfit) led to suspicion that I was myself performing experiments, violating my own thick legitimacy by not following IRB protocol. The result? The author suggests the following in the last paragraph:&lt;/p&gt;
&lt;blockquote&gt;
From a research point of view this is FASCINATING.  I would love to get a hold of the discussion forum data for both discourse and corpus linguistics analyses. On the other hand, I fear that coursera, and all involved parties, are handling this one wrong again.  We are now entering the third and final week of this MOOC on MOOCs.  Let's see how this pans out.&lt;/blockquote&gt;
&lt;p&gt;In other words, the author simultaneously complains about a situation, but wishes he could put his hands on the dataset so he could do research with thick legitimacy. He will of course never have access to that data, since Coursera owns it, from day one. Actually, there are many things that are not very inspiring about the technical architecture of the platform itself, so it might very well be that once an instructor deletes a forum, its content is unrecoverable. That's why I deleted forums when I could: to protect students, despite not having the opportunity to explain exactly why I did it (because of legal risk). Nothing has shown to me since that Coursera was able to retrieve that content.&lt;/p&gt;
&lt;p&gt;The third week of the course was supposed to contain a final, peer-feedback quiz. That quiz would be asking students generic questions about MOOCs and what they learned in the course. The same author talks about that assignment &lt;a class="reference external" href="http://idstuff.blogspot.ch/2014/07/youve-been-punkd-however-that-was.html"&gt;here&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
With regard to this "Assignment" I feel rather cynical on all fronts. On the one hand it feels like this is just another data-gathering stunt. So Paul ran his "experiment" and now he is collecting data to see what the learners say. The learners that didn't un-enroll from the course that is.  On the other hand, even if this is an earnest attempt to have learners introspect on this whole process, the attempt falls really flat on its face because this data is tainted. The questions don't address anything that happened in the course. It feels like these were written with the original learning objectives in mind, and as such it reduces this final exercise into a farce. It is a farce that does not respect the learners, and it is a farce of the educational process.&lt;/blockquote&gt;
&lt;p&gt;It's all of this, actually, and more. This was merely a draft, meant to validate the idea with Coursera staff of data-gathering in this way. But it was &lt;em&gt;also&lt;/em&gt; an earnest attempt to have learners introspect on this whole process, written with the original learning objectives in mind. I intended to tweak this draft later (maybe in a later iteration) into a more formal process, to be able to offer at scale personalised degrees that would reflect the learning that actually happened in the course. Once everything else happened, indeed the data &lt;em&gt;was&lt;/em&gt; tainted and it didn't address anything that happened in the course. The final exercise was reduced into a farce that did not respect the learners. By that time though, I was not involved anymore, and had objected multiple times to running the course and this assignment without me.&lt;/p&gt;
&lt;p&gt;After the assignment was completed by the students, that same author and some commenters on his blog &lt;a class="reference external" href="http://idstuff.blogspot.ch/2014/07/massiveteaching-experiment-falls-on.html"&gt;managed to hack into Coursera&lt;/a&gt; to get access to the data of all the students of the course. They were surprised at the diversity of responses, reflecting their own biases in understanding the complexity of the situation. On top, their interst in this data validated my own concerns: this data, beyond being useful for teaching, was valuable to research on the learner experience yet would be collected without IRB approval (it was merely used for teaching). My own thick legitimacy would enable its collection, yet I would have no option available to protect it from the eyes of Coursera, whose legitimacy is different and who might have genuine interest in that data.&lt;/p&gt;</description><category>coursera</category><category>ethics</category><guid>http://paulolivier.dehaye.org/posts/erosion-of-thick-legitimacy-by-coursera.html</guid><pubDate>Mon, 20 Oct 2014 13:21:14 GMT</pubDate></item><item><title>Learning, working and ?</title><link>http://paulolivier.dehaye.org/posts/learning-working-and.html</link><dc:creator>Paul-Olivier Dehaye</dc:creator><description>&lt;p&gt;A lot of my recent thoughts have turned around issues of crowdsourcing and online education.&lt;/p&gt;
&lt;img alt="../worklearn.jpg" class="align-right" src="http://paulolivier.dehaye.org/worklearn.jpg" style="width: 346.4px; height: 85.6px;"&gt;&lt;p&gt;One of my co-authors (&lt;a class="reference external" href="http://hci.uni-hannover.de/people/markus"&gt;Markus Krause&lt;/a&gt;) is co-organising a workshop on that topic, &lt;a class="reference external" href="http://www.worklearn.org/"&gt;WorkLearn 2014&lt;/a&gt;. It will take place in Pittsburgh November 2-4, as part of the &lt;a class="reference external" href="http://www.humancomputation.com/2014/"&gt;Human Computation conference HCOMP2014&lt;/a&gt;. Human Computation is another expression interchangeable for &lt;a class="reference external" href="http://paulolivier.dehaye.org/posts/social-teaching-machines.html"&gt;social machine&lt;/a&gt;, although it has different connotations.&lt;/p&gt;
&lt;p&gt;The stated motivation of the workshop is very ambitious:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The online education and crowdsourcing communities are addressing similar problems in educating, motivating and evaluating students and workers. The online learning community succeeds in increasing the supply side of the cognitively skilled labor market, and the crowdsourcing at scale community creates a larger marketplace for cognitively skilled work.&lt;/p&gt;
&lt;p&gt;Linking online platforms for crowd work with platforms for MOOCs has the potential to: provide knowledge and training at a massive scale to contributors; collect data that identify expert skills; engage contributors in simultaneously working and learning in a social environment; and organize large communities around online courses on specific topics. These all provide new opportunities to support and deploy sophisticated algorithms for crowd learning and work.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The most successful example in this direction is of course &lt;a class="reference external" href="http://duolingo.com"&gt;Duolingo&lt;/a&gt;, which helps translate the web while using volunteer labor by language learners. If one omits the learning, the strategy there is not that different from the strategy used by my Coursera coworker &lt;a class="reference external" href="https://www.coursera.org/instructor/bernstein"&gt;Abraham Bernstein&lt;/a&gt; to translate books using &lt;a class="reference external" href="http://en.wikipedia.org/wiki/Amazon_Mechanical_Turk"&gt;Amazon Mechanical Turk&lt;/a&gt;, and indeed part of his efforts aim to design effective tools to program those social machines (with the programming language CrowdLang).&lt;/p&gt;
&lt;embed&gt;&lt;iframe width="640" height="360" src="//www.youtube.com/embed/emCABRV2cUA" frameborder="0" allowfullscreen&gt;&lt;/iframe&gt;
&lt;/embed&gt;&lt;p&gt;I have always had some qualms about the &lt;a class="reference external" href="http://florianschmidt.co/the-good-the-bad-and-the-ugly/"&gt;ethics of crowdsourcing&lt;/a&gt;, even though it can clearly be used for good: the prototypical success story is in the work of Ushahidi during the &lt;a class="reference external" href="http://www.ushahidi.com/blog/2012/01/12/haiti-and-the-power-of-crowdsourcing/"&gt;2010 Haiti earthquake&lt;/a&gt;. I was thus very happy to see over Labor Day 2014 that Michael Bernstein from Stanford announced &lt;a class="reference external" href="http://crowdresearch.org/blog/?p=9039"&gt;guidelines for academic requesters on Amazon Mechanical Turk&lt;/a&gt;. He explains the rationale for this (a Turker is a worker on a crowdsourcing platform):&lt;/p&gt;
&lt;blockquote&gt;
An IRB-approved researcher experimented on the [crowdsourcing] platform unannounced. The result was Turker confusion, strife, and wasted time, in a system where time is what it takes to make ends meet.&lt;/blockquote&gt;
&lt;p&gt;These guidelines were themselves crowdsourced, designed together with the Turkers (it's only natural!).&lt;/p&gt;
&lt;p&gt;At the same time, over the summer, there was a huge controversy over the iffy ethics of social platforms experimentation. This is due to the release at the very end of June 2014 of a &lt;a class="reference external" href="http://www.forbes.com/sites/kashmirhill/2014/06/29/facebook-doesnt-understand-the-fuss-about-its-emotion-manipulation-study/"&gt;Facebook experiment on its users&lt;/a&gt; (don't miss the Cornell IRB flowchart there). There are a ton of links about this, but the best is probably the account by Mary L. Gray of an &lt;a class="reference external" href="http://marylgray.org/?page_id=203"&gt;ethics panel that took place at the Microsoft Research Faculty Summit&lt;/a&gt; (and unfortunately was published with much delay).&lt;/p&gt;
&lt;p&gt;In any case, this should give serious pause to any educator. One can see lots of fields suddenly getting much too close, with very different or inexistent values. Online learners, just as Turkers, are vulnerable. &lt;a class="reference external" href="http://nogoodreason.typepad.co.uk/no_good_reason/2014/06/the-ethics-of-digital-scholarship.html"&gt;Martin Weller&lt;/a&gt; and &lt;a class="reference external" href="http://www.elearnspace.org/blog/2014/01/13/the-vulnerability-of-learning/"&gt;George Siemens&lt;/a&gt; have recently insisted on this.&lt;/p&gt;
&lt;p&gt;So, what do you think? Anyone wants to submit a position paper (2 pages) on the topic? Any of my co-learners in MOOCs would like to see what we can do? We could, well... crowdsource it...&lt;/p&gt;
&lt;p&gt;(of course, this was due yesterday: official deadline is "September")&lt;/p&gt;</description><category>connected_course</category><category>coursera</category><category>crowdsourcing</category><category>duolingo</category><category>ethics</category><category>t509massive</category><category>whyopen</category><guid>http://paulolivier.dehaye.org/posts/learning-working-and.html</guid><pubDate>Thu, 11 Sep 2014 00:17:10 GMT</pubDate></item><item><title>"Don't be evil", or how I learned to behave like a startup and love the data</title><link>http://paulolivier.dehaye.org/posts/dont-be-evil-or-how-i-learned-to-behave-like-a-startup-and-love-the-data.html</link><dc:creator>Paul-Olivier Dehaye</dc:creator><description>&lt;img alt="../strangelove.png" class="align-right" src="http://paulolivier.dehaye.org/strangelove.png" style="width: 320.0px; height: 240.0px;"&gt;&lt;p&gt;When Gmail was opened in 2004, I received invitations early. If I remember well, they came from a friend working at Google who had already snatched a few fun login names. I did the same, and passed on further invitations to my brother and our friends back home.&lt;/p&gt;
&lt;p&gt;A year or so later, when my brother was visiting with his friends, we went on a tour of the Googleplex. Randomly passing in front of the cubicle of a homonym, one of the friends suddenly realised why he had not been able to register his own name earlier. In other words, &lt;strong&gt;an unknown collision in the physical world had first manifested digitally&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;I like to think of those collisions as the digital equivalent of New York overcrowding, trying to fit too many people in just a few login characters.&lt;/p&gt;
&lt;p&gt;So which fun pseudonyms did we chose? Which did we consider worthy in this land grab? Certainly many of them were aimed at our shared cultural backgrounds as Belgians in the Silicon Valley. If you had &lt;a class="reference external" href="mailto:tintin@gmail.com"&gt;tintin@gmail.com&lt;/a&gt;, or &lt;a class="reference external" href="http://nl.wikipedia.org/wiki/Frietkot"&gt;frietkot@gmail.com&lt;/a&gt; that would be pretty impressive, no? Indeed, we grabbed names of regions, superheroes, movie stars, concepts, etc. We certainly thought this was OK, and didn't reflect more on something that &lt;a class="reference external" href="http://en.wikipedia.org/wiki/Nymwars"&gt;became controversial only later&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;One of the logins I grabbed had the name of a Belgian politician, let's call him Some Guy. He was on TV and I thought my friends would get a chuckle if I emailed them from it. Certainly, I might have crossed a moral line already then, but it felt like a very tiny escalation in this virtual land grab.&lt;/p&gt;
&lt;p&gt;What did I do with this account? I mostly used it for spam protection. I set it up so that all emails sent there would be forwarded to my default inbox, and gave this address whenever there was a need to a register for a spammy online service. This worked well, possibly because Gmail's algorithms had learned to weigh emails transiting through this address differently and benefited from the additional segmenting.&lt;/p&gt;
&lt;p&gt;Around 2008, inevitably, I started receiving emails addressed to That Guy. Those collisions happen to all of us, for all of our email accounts. What is the moral thing to do there? My philosophy is most of the time to let it drop, but  sometimes also to reply to the sender telling them that they got the wrong address (due to emails missent to my main account, I must have had to contact a dozen hotels in Quebec by now). In most cases, the only way to know what to do is to read the email, slightly invading this other persons' privacy.&lt;/p&gt;
&lt;blockquote&gt;
Just like Rachel and her Friends in their New York apartment, we struggle to deal with those privacy collisions, especially when we feel a need to intervene.&lt;/blockquote&gt;
&lt;iframe width="900" height="600" src="//www.youtube.com/embed/tYn8s0_kDUw?rel=0&amp;amp;hd=1&amp;amp;wmode=transparent"&gt;&lt;/iframe&gt;&lt;p&gt;For That Guy, it was even easier to feel morally OK about it: I never actively sought the emails, had no way to prevent the mistake, and anyways the emails were from cranks. On top, by that time I had registered to too many services with that pseudonym, which effectively tied my identity to it, with no way to revert the situation. So in effect this data collection was happening, whether I liked it or not, or at least that was my moral justification.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The problem with data is that it leaks.&lt;/strong&gt; The cranks don't just email one influential person at a time. They email a few, who are susceptible to know each other. As a consequence, in this case, the cranks polluted those recipients' email software  with a wrong email address. Of course, in due time, the email autocompletion software of those recipients started tripping them and I started receiving emails from other politicians to That Guy. &lt;strong&gt;Algorithmic curation had gone wrong, and actively mislead humans.&lt;/strong&gt; The fact that these were politicians might have mislead me: I should have made the effort of explaining the awkward situation to That Guy's interlocutors and tried to correct it. But I didn't. Somehow a couple more emails made it to me that were clearly of more social nature. Again, I didn't do anything. &lt;strong&gt;This data will not disappear unless actively deleted&lt;/strong&gt;, and even then I can only be so sure.&lt;/p&gt;
&lt;blockquote&gt;
At this point you will deservedly think that I am a moron. But was it morally wrong? And when exactly did it go wrong?&lt;/blockquote&gt;
&lt;p&gt;Throughout my moral justification was that I was not actively seeking this. Emails would land in my mailbox and I would have to read them to know what to do. Of course, this conveniently ignores what I could have done to prevent those emails to arrive in the first place. Part of my justification was that I wasn't doing anything with the data collected. There was no clear goal, except &lt;strong&gt;awareness that this could be used to make a point later, which I guess I am making here now publicly&lt;/strong&gt; (in fact, I have used this to make this point in private throughout the years).&lt;/p&gt;
&lt;p&gt;The more interesting issue here is to understand that this is exactly how many big data companies function. "Don't be evil" Google gobbles data all over the place &lt;a class="reference external" href="http://www.wired.com/2014/04/threatlevel_0401_streetview/"&gt;for purposes that are not always clear at the time&lt;/a&gt;, and the justification is often that this was incidental, automated and did not require human intervention. Looking at a corporate setting elevates the stakes, and my feeble moral justifications are not sufficient anymore. It becomes a matter of ethics, which arguably should be that data collection is by default unethical: data should not be kept beyond the time necessary for its intended use, with that use itself subject to precise and established ethical rules. It looks like Google has understood this in some markets, for instance education (unlike other players there), and this will be the topic of a later post.&lt;/p&gt;
&lt;p&gt;(Image in the public domain: the Dr Stangelove War Room, which &lt;a class="reference external" href="http://valleywag.gawker.com/airbnbs-office-has-a-replica-of-the-dr-strangelove-wa-1475788543"&gt;happens to be replicated in the Airbnb HQ&lt;/a&gt;)&lt;/p&gt;</description><category>ethics</category><category>privacy</category><guid>http://paulolivier.dehaye.org/posts/dont-be-evil-or-how-i-learned-to-behave-like-a-startup-and-love-the-data.html</guid><pubDate>Mon, 08 Sep 2014 09:45:51 GMT</pubDate></item><item><title>Keeping a Soul in the Driver's Seat</title><link>http://paulolivier.dehaye.org/posts/keeping-a-soul-in-the-driver-seat.html</link><dc:creator>Paul-Olivier Dehaye</dc:creator><description>&lt;img alt="../railway.jpg" class="align-right" src="http://paulolivier.dehaye.org/railway.jpg"&gt;&lt;p&gt;I can't wait for driverless cars. Ten years is the estimate. Combined with car sharing, they will revolutionise our cities and make them much more efficient and livable. They should bring about umitigated good to our society. Yet they still come with ethical challenges, usually categorized as &lt;a class="reference external" href="http://en.wikipedia.org/wiki/Trolley_problem"&gt;trolley problems&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Wired just published an article called &lt;a class="reference external" href="http://www.wired.com/2014/08/heres-a-terrible-idea-robot-cars-with-adjustable-ethics-settings/"&gt;Here's a Terrible Idea: Robot Cars With Adjustable Ethics Settings&lt;/a&gt;, outlining the ethical issues involved in substituting human drivers with robots.&lt;/p&gt;
&lt;blockquote&gt;
In freak accidents, computers would have to take decisions such as &lt;strong&gt;killing one motorcyclist without a helmet vs. killing five pedestrians&lt;/strong&gt;.&lt;/blockquote&gt;
&lt;p&gt;The writer raises many such dystopian choices: &lt;strong&gt;children vs elderly&lt;/strong&gt;, &lt;strong&gt;us vs others&lt;/strong&gt;, &lt;strong&gt;rich vs poor&lt;/strong&gt;, etc. He rightfully sees a liability for anyone having to program those decisions. In his opinion, any attempt by the car manufacturer to distantiate itself from lawsuits by offering variable ethical settings to the owner of the car would not decrease the liability of the car manufacturer, and therefore this remains an obstacle to rolling in a driverless car.&lt;/p&gt;
&lt;blockquote&gt;
The car company has another option, which is missed by the writer: absorb progressively the insurance business.&lt;/blockquote&gt;
&lt;p&gt;First off, it's clear that any level of indirection and legal tangling is helpful in freak legal confrontations to shield car manufacturer from legal responsibility towards private individuals. Secondly, the writer does not give enough credit to the creativity of engineers/lawyers/business types.&lt;/p&gt;
&lt;blockquote&gt;
Why wouldn't they be able to introduce one further level of indirection? The manufacturer could build a "car without a soul".&lt;/blockquote&gt;
&lt;p&gt;The car could offer full access proprietary APIs to its raw or slightly processed data, but require linking to an ethical core library before it would start. This ethical core would only be called upon if a future collision is detected, and asked to respond to the really tough questions (or it could be run on a loop validating any driving input). Who would take the liability of writing such a core? Insurance companies would seem like the natural candidate. In fact, this is a very natural extension of their business, litigating for the choices they have coldly programmed in rather than the mistakes made by their clients. It would also make sense to decentralise geographically this ethical core, since driving customs are bound to vary from country to country (think of these &lt;a class="reference external" href="https://www.youtube.com/watch?v=7vd_OuqUAaI"&gt;comparatively safe Indian drivers&lt;/a&gt; or this &lt;a class="reference external" href="https://www.youtube.com/watch?v=RjrEQaG5jPM"&gt;Russian ninja&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;The question is whether insurance companies would be willing to go along. They would certainly feel pressure to adapt to a world of driverless cars, but the brilliant move for the car company would be to promise increased efficiency and reach to the whole insurance industry (more clients), and act as a middleman. By encouraging collaboration between the insurance companies, ostensibly to help them save money on R&amp;amp;D, standardise good practice, exchange regulatory tips, etc, the car company would crowdsource the insurance industry to force itself into obsolescence. This would allow the car company to eventually provide the full product, once all the R&amp;amp;D costs of the fine tuning of the ethical core have been shouldered by the insurance companies. Note that this core would only be ethical in name, as it would have been exclusively fine tuned with cost efficiency in mind.&lt;/p&gt;
&lt;p&gt;This assumes there is a car company that is sufficiently dominating the car industry to strong arm insurance companies.&lt;/p&gt;
&lt;p&gt;(For other futuristic and "fun" questions on the transformation brought about by driverless cars, see the amazingly cold-blooded &lt;a class="reference external" href="http://fortune.com/2014/08/15/if-driverless-cars-save-lives-where-will-we-get-organs/"&gt;If driverless cars save lives, where will we get organs?&lt;/a&gt;.)&lt;/p&gt;
&lt;p&gt;(Image credit: Wikipedia)&lt;/p&gt;</description><category>crowdsourcing</category><category>ethics</category><guid>http://paulolivier.dehaye.org/posts/keeping-a-soul-in-the-driver-seat.html</guid><pubDate>Fri, 22 Aug 2014 09:16:37 GMT</pubDate></item></channel></rss>