<p>There was recently a <a class="reference external" href="https://twitter.com/AndrewBRElliott/status/507912025599934464/photo/1">picture circulating on Twitter</a>, like pictures do.</p>
<img alt="../why_google.jpg" class="align-center" src="../why_google.jpg" style="width: 618.0px; height: 564.0px;" />
<p>This is, to say the least, a skewed view of academia, although I am certainly not the best placed to say that. I tend to have a beard, use big words, have recently started blogging and did <a class="reference external" href="http://en.wikipedia.org/wiki/Academic_dress_of_the_University_of_Oxford">wear robes at some point in my academic career</a>. This is however a good opportunity to show how algorithmic bias works.</p>
<p>First off, where does the bias originate here?
As I explained before in my post on <a class="reference external" href="../posts/social-teaching-machines.html">social teaching machines</a>, autocompletion surfaces information collected previously. The information in this case is collected in various ways, most notably by looking at previous searches. I would posit that the way the information is collected leads itself to the bias. Some of those biases are more serious, as is amply demonstrated by any search of the form &quot;Why do A people like&quot;, where A can be any of {asian, white, black}. Only one of those autocompletes works for me, the other two give nothing (presumably because the output is too vile and has been hand blocked). So these stereotypes, surfaced by humans, are inserted into the huge Google machine. Let's see where this leads.</p>
<p>In the case of autocompletion, the impact is certainly weak, but it might correlate with other biases, underscoring a more ingrained problem. Let's go back to Google's view of academia: what does the output of a <a class="reference external" href="https://www.google.com/search?site=&amp;tbm=isch&amp;source=hp&amp;biw=1366&amp;bih=635&amp;q=academics+faculty&amp;oq=academics+faculty">Google Image Search of &quot;academics faculty&quot;</a> return? The link in the previous sentence is user agnostic (but its output will be personalized by Google once you click, unless you use privacy conscious tools). Here is the view I get:</p>
<img alt="../why_google_images.jpg" class="align-center" src="../why_google_images.jpg" style="width: 1340.0px; height: 611.0px;" />
<p>Yours should be different: most probably, Terence Tao, the short-sleeved mathematician in the middle row is further down in yours. This is reasonable, and explained by Christian Sandvig in the Social Media Collective blog in a beautiful post called <a class="reference external" href="http://socialmediacollective.org/2014/03/25/show-and-tell-algorithmic-culture/">Show-and-Tell: Algorithmic Culture</a>: since I am a mathematician, Google gives Tao a bump <a class="footnote-reference" href="#id2" id="id1">[1]</a>. And beyond that? Well, Google <em>really</em> thinks that academics wear robes, and perpetuates this bias also visually, not just in autocomplete. One consequence is that when humans need to illustrate something (a blog, an educational resource,...) it actually requires training and effort not to succumb to that bias. And of course this intermediate step of choosing a picture could itself be automated in various ways.</p>
<table class="docutils footnote" frame="void" id="id2" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id1">[1]</a></td><td>This effect, of showing me Tao higher than average, might be reasonable in this case, but it could also lead to some form of <a class="reference external" href="http://en.wikipedia.org/wiki/Filter_bubble">filter bubble</a> if there were (many) more mathematicians.</td></tr>
</tbody>
</table>
