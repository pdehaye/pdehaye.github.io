<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>paulolivier.dehaye.org</title><link>http://paulolivier.dehaye.org/</link><description>Opinions are my own</description><language>en</language><lastBuildDate>Fri, 12 Sep 2014 11:58:01 GMT</lastBuildDate><generator>http://getnikola.com/</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Edtech policies (part II)</title><link>http://paulolivier.dehaye.org/posts/edtech-policies-part-ii.html</link><dc:creator>Paul-Olivier Dehaye</dc:creator><description>&lt;p&gt;In his recent post &lt;em&gt;You are not alone&lt;/em&gt; on the Salaita affair, Jonathan Rees &lt;a class="reference external" href="http://academeblog.org/2014/09/08/you-are-not-alone/"&gt;comments on the social media policies&lt;/a&gt; enacted at the Colorado, Kentucky and New Mexico state systems. He encourages other professors to speak up:&lt;/p&gt;
&lt;blockquote class="epigraph"&gt;
&lt;p&gt;Few of us can be sure whether or not our campus is really the canary in the higher education coal mine. However, better mine safety everywhere eventually benefits everyone with a pick and shovel. The more we professors talk to each other, both on social media and off, the better off we’ll all be when bad ideas surface that may someday affect us all.&lt;/p&gt;
&lt;p class="attribution"&gt;—Jonathan Rees&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The evocation of the canary is very beautiful. It resonates quite a bit: &lt;a class="reference external" href="https://www.ethz.ch/en.html"&gt;ETH Zurich&lt;/a&gt; was after all founded to help the Swiss develop their own railway systems. In any case, looking back at his writing, Rees has really dug in the Coursera &lt;a class="reference external" href="http://moreorlessbunk.wordpress.com/2013/09/01/dear-superprofessors-your-labor-has-value/"&gt;labor&lt;/a&gt; &lt;a class="reference external" href="http://moreorlessbunk.wordpress.com/2013/09/03/unbundle-yourself/"&gt;practices&lt;/a&gt;. This seems like a good opportunity to continue my &lt;a class="reference external" href="http://paulolivier.dehaye.org/posts/edtech-policies-part-i.html"&gt;edtech policies series&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;After some googling, one can easily find the &lt;a class="reference external" href="http://www.colorado.edu/bfa/INITIATIVES/CourseraK2013.pdf"&gt;Coursera Framework Agreement&lt;/a&gt; between &lt;em&gt;Coursera Inc., a Delaware corporation [..] and the Board of Regents of the University of Colorado [..] for the benefit of the University of Colorado Boulder&lt;/em&gt;.&lt;/p&gt;
&lt;img alt="../colorado-google.jpg" class="align-center" src="http://paulolivier.dehaye.org/colorado-google.jpg" style="width: 545.0px; height: 123.0px;"&gt;&lt;p&gt;Independently of any stereotype that as an academic &lt;a class="reference external" href="http://paulolivier.dehaye.org/posts/the-academic-social-machine-part-I.html"&gt;I hate capitalism&lt;/a&gt;, another stereotype could be that I also quite like &lt;a class="reference external" href="https://www.facebook.com/notes/paul-olivier-dehaye/insane-travel-insurance-policy/228313430077"&gt;to read contracts&lt;/a&gt;. Of course this one makes for a particularly interesting read: it is supposed to hint at how MOOCs will be sustainable for the UC system and how Coursera, Inc. will outlast its &lt;a class="reference external" href="http://www.crunchbase.com/organization/coursera"&gt;85M dollars "gifts" from venture capital&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The contract details the various rights and obligations of the parties involved, the different Coursera monetization strategies (ten instead of the eight like in the &lt;a class="reference external" href="http://s3.documentcloud.org/documents/400864/coursera-fully-executed-agreement.pdf"&gt;2012 Michigan contract&lt;/a&gt;), the Course Acceptance Procedures, the responsibility of the &lt;a class="reference external" href="http://blog.coursera.org/post/72903326640/introducing-the-2014-rotation-of-courseras-university"&gt;University Advisory Board&lt;/a&gt; when things go wrong, etc. Of particular interest to me is how it already packages in Exhibit G-1 the relationship between an instructor and the university.&lt;/p&gt;
&lt;p&gt;It also includes the following gem:&lt;/p&gt;
&lt;blockquote class="epigraph"&gt;
&lt;p&gt;I hereby release, discharge, and promise not to sue Company and its affiliates, successors and assigns from and against any and all claims, demands, costs and/or causes of actions of any nature arising out of or in connection with the exercise of any rights herein granted, including, without limitation, any claim for infringement, right of publicity, libel, slander, defamation, moral rights, invasion of privacy or violation of any other rights relating to any Content I upload, share or otherwise provide in connection with use of the Platform.&lt;/p&gt;
&lt;p class="attribution"&gt;—Exhibit G-1, Form of Agreement for Instructors and Guest Presenters&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In other words, IANAL but once the instructor signs this, s/he is giving free pass to Coursera for any libel, slander, defamation, invasion of privacy etc against her/himself. I don't know the backstory on how the faculty at Colorado accepted this, but would be very interested. At UCSC, the collective bargaining chapter has on the other hand been &lt;a class="reference external" href="http://ucscfa.org/2013/06/scfas-ongoing-discussion-concerning-ucscs-contract-with-coursera/"&gt;very critical&lt;/a&gt;, as has been discussed in &lt;a class="reference external" href="http://paulolivier.dehaye.org/posts/edtech-policies-part-i.html"&gt;part I&lt;/a&gt; of this series.&lt;/p&gt;
&lt;p&gt;Rees' words on this are very apt, to his credit a year ago already:&lt;/p&gt;
&lt;blockquote class="epigraph"&gt;
&lt;p&gt;At some point you have to realize that you do not work in a vacuum. Teaching a MOOC or not teaching a MOOC, speaking out in favor of them or remaining silent – these things all have an effect on the rest of your profession. You can’t just declare that MOOCs are the future and let the chips fall where they may. You have the ability to influence what kind of future we’ll all face.&lt;/p&gt;
&lt;p class="attribution"&gt;—Jonathan Rees (&lt;a class="reference external" href="https://moreorlessbunk.wordpress.com/2013/09/01/dear-superprofessors-your-labor-has-value/"&gt;Labor Day 2013&lt;/a&gt;)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Can we guess what XXIst century education might guarantee in terms of labor rights? Is there an online strike clause in any of them? All I could find is called &lt;em&gt;Force Majeure&lt;/em&gt;:&lt;/p&gt;
&lt;blockquote class="epigraph"&gt;
&lt;p&gt;Each party is excused from performance of this Agreement (other than for any payments due) and will not be liable for any delay in whole or in part caused by the occurrence of any contigency beyond the reasonable control of such Party. These contigencies include, without limitation, war, sabotage, insurrection, riot or other acts of civil disobedience, act of public enemy, failure or delay in transportation, act of government or any agency or subdivision thereof affecting the terms of this Agreement or otherwise, judicial action, &lt;strong&gt;labor dispute&lt;/strong&gt;, student disorders, accident, fire, explosion, flood, severe weather, natural disaster or other &lt;strong&gt;act of God&lt;/strong&gt;, shortage of labor, hardware failure, interruptions or &lt;strong&gt;failure of the Internet&lt;/strong&gt; or third-party network connections or incapacity of an instructor.&lt;/p&gt;
&lt;p class="attribution"&gt;—&lt;em&gt;Force majeure&lt;/em&gt; clause 19.6 of University of Colorado Boulder contract (emphasis mine)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I don't know if those risks are sorted in any way, but can't help but notice that &lt;em&gt;act of God&lt;/em&gt; sits right between
&lt;em&gt;labor dispute&lt;/em&gt; and &lt;em&gt;failure of the Internet&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Apparently Rees has a show on the road. The title is &lt;a class="reference external" href="https://moreorlessbunk.wordpress.com/category/academia/aaup/"&gt;Educational Technology, Budgetary Priorities and Academic Freedom&lt;/a&gt;. Should be good.&lt;/p&gt;</description><category>coursera</category><category>crowdsourcing</category><guid>http://paulolivier.dehaye.org/posts/edtech-policies-part-ii.html</guid><pubDate>Thu, 11 Sep 2014 21:16:55 GMT</pubDate></item><item><title>The academic (social) machine (part I)</title><link>http://paulolivier.dehaye.org/posts/the-academic-social-machine-part-I.html</link><dc:creator>Paul-Olivier Dehaye</dc:creator><description>&lt;p&gt;There was recently a &lt;a class="reference external" href="https://twitter.com/AndrewBRElliott/status/507912025599934464/photo/1"&gt;picture circulating on Twitter&lt;/a&gt;, like pictures do.&lt;/p&gt;
&lt;img alt="../why_google.jpg" class="align-center" src="http://paulolivier.dehaye.org/why_google.jpg" style="width: 370.8px; height: 338.4px;"&gt;&lt;br&gt;&lt;p&gt;This is, to say the least, a skewed view of academia, although I am certainly not the best placed to say that. I tend to have a beard, use big words, have recently started blogging and did &lt;a class="reference external" href="http://en.wikipedia.org/wiki/Academic_dress_of_the_University_of_Oxford"&gt;wear robes at some point in my academic career&lt;/a&gt;. This is however a good opportunity to show how algorithmic bias works.&lt;/p&gt;
&lt;p&gt;First off, where does the bias originate here?
As I explained before in my post on &lt;a class="reference external" href="http://paulolivier.dehaye.org/posts/social-teaching-machines.html"&gt;social teaching machines&lt;/a&gt;, autocompletion surfaces information collected previously. The information in this case is collected in various ways, most notably by looking at previous searches. It is difficult to make broad statements on why people make a Google search. It's an act that always occurs in a certain context, for a ton of different reasons, and Google optimises for an average that is unclear. Who else is making the same search will undoubtedly have an effect on how Google sees the context of a search. Consider the &lt;a class="reference external" href="https://www.google.ch/?gfe_rd=cr&amp;amp;ei=uvERVIGPKKbC8gfCuIGQAg&amp;amp;gws_rd=ssl#q=why+do+english+people+have+british+humor"&gt;search&lt;/a&gt; &lt;em&gt;Why do English people have British humor?&lt;/em&gt; and its first answer: Google obviously got the context wrong in my case.&lt;/p&gt;
&lt;br&gt;&lt;img alt="../british_humor.jpg" class="align-center" src="http://paulolivier.dehaye.org/british_humor.jpg" style="width: 546.4px; height: 196.8px;"&gt;&lt;br&gt;&lt;p&gt;I am a bit at a loss to say more on this, so feel free to comment.&lt;/p&gt;
&lt;p&gt;In any case, some of those biases are substantially more serious, of course. For this, you simply have to enter a search of the form "Why do &lt;strong&gt;A&lt;/strong&gt; people &lt;strong&gt;B&lt;/strong&gt;", where &lt;strong&gt;A&lt;/strong&gt; can be any of {&lt;em&gt;asian&lt;/em&gt;, &lt;em&gt;white&lt;/em&gt;, &lt;em&gt;black&lt;/em&gt;} and &lt;strong&gt;B&lt;/strong&gt; any of {&lt;em&gt;look&lt;/em&gt;, &lt;em&gt;like&lt;/em&gt; &lt;em&gt;smell&lt;/em&gt;} to realise that autocomplete is powerful to surface common stereotypes. Not all those autocompletes work though (presumably because the output is too vile and has been hand blocked). So we humans enter our biases, and Google actually amplifies them.&lt;/p&gt;
&lt;p&gt;Let's see where this leads.&lt;/p&gt;
&lt;p&gt;In the case of autocompletion, the impact is certainly weak, but it might correlate with other biases (or cause them?), underscoring a more ingrained problem. Let's go back to Google's view of academia: what does the output of a Google Image Search of &lt;em&gt;academics faculty&lt;/em&gt; return?  You can try to use this &lt;a class="reference external" href="https://www.google.com/search?site=&amp;amp;tbm=isch&amp;amp;source=hp&amp;amp;biw=1366&amp;amp;bih=635&amp;amp;q=academics+faculty&amp;amp;oq=academics+faculty"&gt;link&lt;/a&gt;, which is user agnostic (but its output will be personalized by Google once you click, unless you use privacy conscious tools). Here is the view I get, when logged in:&lt;/p&gt;
&lt;img alt="../why_google_images.jpg" class="align-center" src="http://paulolivier.dehaye.org/why_google_images.jpg" style="width: 1340.0px; height: 611.0px;"&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;Yours should be different: most probably, &lt;a class="reference external" href="http://en.wikipedia.org/wiki/Terence_Tao"&gt;Terence Tao&lt;/a&gt;, the short-sleeved mathematician in the middle row, is further down in yours. This is reasonable, and explained by &lt;a class="reference external" href="http://www-personal.umich.edu/~csandvig/"&gt;Christian Sandvig&lt;/a&gt; in a beautiful post called &lt;a class="reference external" href="http://socialmediacollective.org/2014/03/25/show-and-tell-algorithmic-culture/"&gt;Show-and-Tell: Algorithmic Culture&lt;/a&gt;: since I am a mathematician, Google gives Tao a bump &lt;a class="footnote-reference" href="http://paulolivier.dehaye.org/posts/the-academic-social-machine-part-I.html#id2" id="id1"&gt;[1]&lt;/a&gt;. And beyond that? Well, Google &lt;em&gt;really&lt;/em&gt; thinks that academics wear robes half the time, and perpetuates this bias also visually, not just in autocomplete.&lt;/p&gt;
&lt;p&gt;Is this really a serious problem? Well, one consequence is that when humans need to illustrate something (a blog, an educational resource,...) it actually requires effort, judgement to accurately assess the potential bias and not succumb to it. For robes it is of course very easy. For skin color or diversity, as we know, it can be harder and thus requires training and conscious effort. In its outputs, Google is &lt;em&gt;potentially&lt;/em&gt; already biased, in ways that are hard to assess for all of us who don't know the Google secret sauce.&lt;/p&gt;
&lt;p&gt;Automation is moving rapidly in many areas. Online advertising could easily remove any intermediate human step, refeeding to us our own biases for commercial gain. Services like &lt;a class="reference external" href="http://www.ece.nus.edu.sg/stfpage/eletp/Projects/Sketch2Photo/"&gt;Sketch2Photo&lt;/a&gt; offer the promise of automating the illustration processes, for instance for lecture notes, which would perpetuate this same effect over more vulnerable populations.&lt;/p&gt;
&lt;table class="docutils footnote" frame="void" id="id2" rules="none"&gt;&lt;colgroup&gt;&lt;col class="label"&gt;&lt;col&gt;&lt;/colgroup&gt;&lt;tbody valign="top"&gt;&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="http://paulolivier.dehaye.org/posts/the-academic-social-machine-part-I.html#id1"&gt;[1]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;The fact that Tao is bumped higher than average for me is probably good. It makes the output more relevant to me. If overdone it could also lead to some form of &lt;a class="reference external" href="http://en.wikipedia.org/wiki/Filter_bubble"&gt;filter bubble&lt;/a&gt;.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;</description><category>algorithmic_bias</category><category>google</category><category>social_machine</category><guid>http://paulolivier.dehaye.org/posts/the-academic-social-machine-part-I.html</guid><pubDate>Thu, 11 Sep 2014 11:40:33 GMT</pubDate></item><item><title>ašţal</title><link>http://paulolivier.dehaye.org/posts/astal.html</link><dc:creator>Paul-Olivier Dehaye</dc:creator><description>&lt;blockquote class="epigraph"&gt;
&lt;p&gt;Well, no language, as far as I know, has a single word for that chin-stroking moment you get, often accompanied by a frown on your face, when someone expresses an idea that you’ve never thought of and you have a moment of suddenly seeing possibilities you never saw before.&lt;/p&gt;
&lt;p&gt;In Ithkuil, it’s ašţal.&lt;/p&gt;
&lt;p class="attribution"&gt;—&lt;a class="reference external" href="http://www.newyorker.com/magazine/2012/12/24/utopian-for-beginners"&gt;John Quijada&lt;/a&gt;, Department of Motor Vehicles&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;That's what we are going for here.&lt;/p&gt;</description><guid>http://paulolivier.dehaye.org/posts/astal.html</guid><pubDate>Thu, 11 Sep 2014 10:41:53 GMT</pubDate></item><item><title>Learning, working and ?</title><link>http://paulolivier.dehaye.org/posts/learning-working-and.html</link><dc:creator>Paul-Olivier Dehaye</dc:creator><description>&lt;p&gt;A lot of my recent thoughts have turned around issues of crowdsourcing and online education.&lt;/p&gt;
&lt;img alt="../worklearn.jpg" class="align-right" src="http://paulolivier.dehaye.org/worklearn.jpg" style="width: 346.4px; height: 85.6px;"&gt;&lt;p&gt;One of my co-authors (&lt;a class="reference external" href="http://hci.uni-hannover.de/people/markus"&gt;Markus Krause&lt;/a&gt;) is co-organising a workshop on that topic, &lt;a class="reference external" href="http://www.worklearn.org/"&gt;WorkLearn 2014&lt;/a&gt;. It will take place in Pittsburgh November 2-4, as part of the &lt;a class="reference external" href="http://www.humancomputation.com/2014/"&gt;Human Computation conference HCOMP2014&lt;/a&gt;. Human Computation is another expression interchangeable for &lt;a class="reference external" href="http://paulolivier.dehaye.org/posts/social-teaching-machines.html"&gt;social machine&lt;/a&gt;, although it has different connotations.&lt;/p&gt;
&lt;p&gt;The stated motivation of the workshop is very ambitious:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The online education and crowdsourcing communities are addressing similar problems in educating, motivating and evaluating students and workers. The online learning community succeeds in increasing the supply side of the cognitively skilled labor market, and the crowdsourcing at scale community creates a larger marketplace for cognitively skilled work.&lt;/p&gt;
&lt;p&gt;Linking online platforms for crowd work with platforms for MOOCs has the potential to: provide knowledge and training at a massive scale to contributors; collect data that identify expert skills; engage contributors in simultaneously working and learning in a social environment; and organize large communities around online courses on specific topics. These all provide new opportunities to support and deploy sophisticated algorithms for crowd learning and work.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The most successful example in this direction is of course &lt;a class="reference external" href="http://duolingo.com"&gt;Duolingo&lt;/a&gt;, which helps translate the web while using volunteer labor by language learners. If one omits the learning, the strategy there is not that different from the strategy used by my Coursera coworker &lt;a class="reference external" href="https://www.coursera.org/instructor/bernstein"&gt;Abraham Bernstein&lt;/a&gt; to translate books using &lt;a class="reference external" href="http://en.wikipedia.org/wiki/Amazon_Mechanical_Turk"&gt;Amazon Mechanical Turk&lt;/a&gt;, and indeed part of his efforts aim to design effective tools to program those social machines (with the programming language CrowdLang).&lt;/p&gt;
&lt;embed&gt;&lt;iframe width="640" height="360" src="//www.youtube.com/embed/emCABRV2cUA" frameborder="0" allowfullscreen&gt;&lt;/iframe&gt;
&lt;/embed&gt;&lt;p&gt;I have always had some qualms about the &lt;a class="reference external" href="http://florianschmidt.co/the-good-the-bad-and-the-ugly/"&gt;ethics of crowdsourcing&lt;/a&gt;, even though it can clearly be used for good: the prototypical success story is in the work of Ushahidi during the &lt;a class="reference external" href="http://www.ushahidi.com/blog/2012/01/12/haiti-and-the-power-of-crowdsourcing/"&gt;2010 Haiti earthquake&lt;/a&gt;. I was thus very happy to see over Labor Day 2014 that Michael Bernstein from Stanford announced &lt;a class="reference external" href="http://crowdresearch.org/blog/?p=9039"&gt;guidelines for academic requesters on Amazon Mechanical Turk&lt;/a&gt;. He explains the rationale for this (a Turker is a worker on a crowdsourcing platform):&lt;/p&gt;
&lt;blockquote&gt;
An IRB-approved researcher experimented on the [crowdsourcing] platform unannounced. The result was Turker confusion, strife, and wasted time, in a system where time is what it takes to make ends meet.&lt;/blockquote&gt;
&lt;p&gt;These guidelines were themselves crowdsourced, designed together with the Turkers (it's only natural!).&lt;/p&gt;
&lt;p&gt;At the same time, over the summer, there was a huge controversy over the iffy ethics of social platforms experimentation. This is due to the release at the very end of June 2014 of a &lt;a class="reference external" href="http://www.forbes.com/sites/kashmirhill/2014/06/29/facebook-doesnt-understand-the-fuss-about-its-emotion-manipulation-study/"&gt;Facebook experiment on its users&lt;/a&gt; (don't miss the Cornell IRB flowchart there). There are a ton of links about this, but the best is probably the account by Mary L. Gray of an &lt;a class="reference external" href="http://marylgray.org/?page_id=203"&gt;ethics panel that took place at the Microsoft Research Faculty Summit&lt;/a&gt; (and unfortunately was published with much delay).&lt;/p&gt;
&lt;p&gt;In any case, this should give serious pause to any educator. One can see lots of fields suddenly getting much too close, with very different or inexistent values. Online learners, just as Turkers, are vulnerable. &lt;a class="reference external" href="http://nogoodreason.typepad.co.uk/no_good_reason/2014/06/the-ethics-of-digital-scholarship.html"&gt;Martin Weller&lt;/a&gt; and &lt;a class="reference external" href="http://www.elearnspace.org/blog/2014/01/13/the-vulnerability-of-learning/"&gt;George Siemens&lt;/a&gt; have recently insisted on this.&lt;/p&gt;
&lt;p&gt;So, what do you think? Anyone wants to submit a position paper (2 pages) on the topic? Any of my co-learners in MOOCs would like to see what we can do? We could, well... crowdsource it...&lt;/p&gt;
&lt;p&gt;(of course, this was due yesterday: official deadline is "September")&lt;/p&gt;</description><category>connected_course</category><category>coursera</category><category>crowdsourcing</category><category>duolingo</category><category>ethics</category><category>t509massive</category><category>whyopen</category><guid>http://paulolivier.dehaye.org/posts/learning-working-and.html</guid><pubDate>Thu, 11 Sep 2014 00:17:10 GMT</pubDate></item><item><title>Click here so I can tell you about privacy (and invade yours too)</title><link>http://paulolivier.dehaye.org/posts/click-here-so-i-can-teach-you-about-privacy-and-invade-it-too.html</link><dc:creator>Paul-Olivier Dehaye</dc:creator><description>&lt;p&gt;Simply by looking at this page, you agree to this site's privacy policy, which is a copy of the Swiss Railway's statement on the use of Google Analytics:&lt;/p&gt;
&lt;blockquote class="epigraph"&gt;
&lt;p&gt;“This website uses Google Analytics, a web analytics service provided by Google, Inc. (“Google”). Google Analytics uses “cookies”, which are text files placed on your computer, to help the website analyze how users use the site. The information generated by the cookie about your use of the website (including your IP address) will be transmitted to and stored by Google on servers in the United States . Google will use this information for the purpose of evaluating your use of the website, compiling reports on website activity for website operators and providing other services relating to website activity and internet usage. Google may also transfer this information to third parties where required to do so by law, or where such third parties process the information on Google's behalf. Google will not associate your IP address with any other data held by Google. You may refuse the use of cookies by selecting the appropriate settings on your browser, however please note that if you do this you may not be able to use the full functionality of this website. By using this website, you consent to the processing of data about you by Google in the manner and for the purposes set out above.”&lt;/p&gt;
&lt;p class="attribution"&gt;—&lt;a class="reference external" href="https://support.google.com/analytics/answer/6004245"&gt;Google Analytics Terms of Service&lt;/a&gt;, as on the &lt;a class="reference external" href="http://www.sbb.ch/en/meta/data-protection/data-protection-google-analytics-statement.html"&gt;SBB website&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The site itself does not collect data. However I do use Google Analytics, which requires me to introduce this policy as part of the service. The service allows me to track users on this website in various ways, and produces beautiful graphics like this one:&lt;/p&gt;
&lt;img alt="../google-analytics.jpg" class="align-center" src="http://paulolivier.dehaye.org/google-analytics.jpg" style="width: 1020.0px; height: 518.0px;"&gt;&lt;p&gt;The comments below are bound by the Disqus Terms of Service, which are &lt;a class="reference external" href="https://help.disqus.com/customer/portal/articles/466259-privacy-policy"&gt;available here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If you don't like it, you can leave. So is the law of the internet.&lt;/p&gt;</description><category>connected_course</category><category>edtech</category><category>privacy</category><category>switzerland</category><guid>http://paulolivier.dehaye.org/posts/click-here-so-i-can-teach-you-about-privacy-and-invade-it-too.html</guid><pubDate>Wed, 10 Sep 2014 06:44:21 GMT</pubDate></item><item><title>Edtech policies (part I)</title><link>http://paulolivier.dehaye.org/posts/edtech-policies-part-i.html</link><dc:creator>Paul-Olivier Dehaye</dc:creator><description>&lt;p&gt;In a previous post, I talked about some of the problems associated with &lt;a class="reference external" href="http://paulolivier.dehaye.org/posts/dont-be-evil-or-how-i-learned-to-behave-like-a-startup-and-love-the-data.html"&gt;data collection with no clear purpose&lt;/a&gt;. In this one, I want to compare two of the big players in edtech on a narrow point, that of comment posts and their associated privacy. This is partly done in response to &lt;a class="reference external" href="https://twitter.com/funnymonkey"&gt;Bill Fitzgerald&lt;/a&gt;'s posts on edtech privacy policies, but also as a concerned parent looking a bit far out in the future.&lt;/p&gt;
&lt;p&gt;On June 26th Google held an I/O developer conference. I was hoping, like many, some kind of announcement about &lt;a class="reference external" href="http://www.mooc.org"&gt;mooc.org&lt;/a&gt;. Not much was said on that, but it was still an instructive watch of the efforts of Google in the MOOC space. I was particularly struck by a comment of Julia Wilkowski (leader in their MOOC project) following a question from the audience. The comment is at 28:35, if it does not play for you:&lt;/p&gt;
&lt;embed&gt;&lt;iframe width="640" height="360" src="//www.youtube.com/embed/YCUZ01yFtsM" frameborder="0" allowfullscreen&gt;&lt;/iframe&gt;
&lt;/embed&gt;&lt;p&gt;She informs us that no sophisticated analysis has been performed thus far on the Google MOOC forum posts. There are apparently two reasons: it is complicated, and they are required to delete all the posts after 60 days to comply with their privacy policy. Indeed, Google MOOCs fall under the global umbrella of the Google Privacy Policy, which seems to uniformly apply to all their products. The main reason for this number seems to be technical rather than anything else (the backup system is presumably very complex, extending all the way to physical tapes), since I couldn't find a reference to it in the Google Privacy Policy (and neither could other people, if you, well, google it).&lt;/p&gt;
&lt;p&gt;A tad later, Peter Norvig talks about classifiers (similar to those used in the recent Facebook experiment that would make the news 3 or 4 days later), for instance to help determine when a student might be confused, a classic trick in intelligent tutoring systems. He immediately reminds us though:&lt;/p&gt;
&lt;blockquote class="epigraph"&gt;
&lt;p&gt;But there still are a lot of privacy issues involved in what [..] information can you keep, how much can you tie the identity in the forum to the identity of the student, can you tie that to their identity someplace else, and the field as a whole has to come to grips with the privacy issues so we can share and learn what we want without violating privacy.&lt;/p&gt;
&lt;p class="attribution"&gt;—Peter Norvig&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Coursera is another big player in the market, with a rather different approach. In their Terms of Service and Privacy policy, one can find the following:&lt;/p&gt;
&lt;img alt="../forum-reuse.jpg" class="align-center" src="http://paulolivier.dehaye.org/forum-reuse.jpg" style="width: 868.0px; height: 112.0px;"&gt;&lt;p&gt;The most remarkable sentence here is: &lt;strong&gt;We also reserve the right to reuse Forum posts containing Personally Identifiable Information in future versions of the course we offer, and to enhance further course offerings.&lt;/strong&gt; This sentence is very puzzling to me. What does it mean? The only logical explanation I can offer is that Coursera plans to repopulate forums in later iterations of the course with posts from a previous run, presumably one where there was more emphasis on moderating the course. Ask a dumb question once, and it will be asked again on repeat, &lt;strong&gt;in your name&lt;/strong&gt;, in contexts you don't necessarily know. Share a bit too much of your great idea for a startup, in a context where you feel confortable, well, too bad: it might be reshared again, even if you delete your post at the end of a course (many MOOC students have the feeble illusion that this protects their intellectual property).&lt;/p&gt;
&lt;p&gt;The Terms of Service also include the following, which is classically &lt;a class="reference external" href="https://www.google.ch/webhp?sourceid=chrome-instant&amp;amp;ion=1&amp;amp;espv=2&amp;amp;ie=UTF-8#q=%22Neither+the+User+Content+(as+defined+below)+on+these+Sites%2C+nor+any+links+to+other+websites%2C+are+screened%2C+moderated%2C+approved%2C+reviewed+or+endorsed%22&amp;amp;start=0"&gt;present in many platform disclaimers&lt;/a&gt;:&lt;/p&gt;
&lt;img alt="../forum-disclaimer.jpg" class="align-center" src="http://paulolivier.dehaye.org/forum-disclaimer.jpg" style="width: 863.0px; height: 156.0px;"&gt;&lt;p&gt;The Coursera Terms of Service include numerous such disturbing clauses, as detailed in &lt;a class="reference external" href="http://www.craigbutosi.ca/blog/coursera-or-socrates-was-not-a-content-provider-the-university-of-toronto-and-coursera-agreement"&gt;this wonderful post&lt;/a&gt; from August 2012. That post is a &lt;strong&gt;very highly recommended read&lt;/strong&gt;, since it details issues of free speech and academic freedom, a hot button those days. It also includes the following gem:&lt;/p&gt;
&lt;blockquote&gt;
It is crucial that we pay close attention to the fine print, something unfortunately overshadowed by the immediacy and novelty of Web 2.0 solutions and the latest trends in brand management techniques.&lt;/blockquote&gt;
&lt;p&gt;I can only recommend this approach, but it has to be moderated by a quote given earlier in the piece:&lt;/p&gt;
&lt;blockquote&gt;
Because Coursera mediates between instructor/university and user/student communication, we are dealing with at least four major relationships: user-Coursera, Coursera-instructor/university, user-Coursera-instructor/university, and vice versa. I am mainly focusing on the user-Coursera relation (terms of use and privacy policy), but it should be noted that these are really only separable at the analytical level. In reality, all of these relations are in play at any given time.&lt;/blockquote&gt;
&lt;p&gt;This leaves many unanswered questions, which are not easy to address. It requires access to other contracts, between the instructor, the university and Coursera itself.&lt;/p&gt;
&lt;p&gt;Only one such contract between a university and Coursera has been discussed on a wide scale, the contract given to the University of Michigan. It was the object of a 2012 Chronicle of Higher Education article (an antiquity in the domain of MOOCs), and is based on a Freedom of Information request for the &lt;a class="reference external" href="http://chronicle.com/article/Document-Examine-the-U-of/133063/"&gt;University of Michigan contracts&lt;/a&gt;.
More recently, the UCSC Faculty Union has entered negociations with Coursera, that are &lt;a class="reference external" href="http://ucscfa.org/2013/06/scfas-ongoing-discussion-concerning-ucscs-contract-with-coursera/"&gt;detailed on its blog&lt;/a&gt;. From the outside, these negociations seem very one-sided and highlight differences with the University of Michigan contract:&lt;/p&gt;
&lt;blockquote&gt;
In the Michigan contract, the instructor grants to COURSERA various rights FOR THE DURATION SUCH CONTENT IS OFFERED THROUGH THE PLATFORM (i.e., very limited transfer  of rights).  In our contract, in contrast, the rights are granted TO THE UNIVERSITY and this appears to be irrevocable and not connected to the hosting of the course on the Coursera  platform.&lt;/blockquote&gt;
&lt;p&gt;In other words, the balance of the Coursera contract shifts towards the instructor at the University of Michigan, compared to at UCSC. In the latest Shangai rankings (for the little they are worth), Michigan was ranked 22nd while UCSC was 93rd. Bear in mind that Michigan joined earlier, which might also affect this complex bargaining equation.&lt;/p&gt;
&lt;p&gt;A few other contracts have been put online, intentionally or not, and can be found by googling titles etc. I found eight in total, which can serve as evidence of subtle shifts of the Coursera strategy, and also segmentation according to the characteristics of the universities involved (European vs. American, public vs. private, etc). Since many of those contracts are locked under Non Disclosure Agreements, and this has already become a union issue elsewhere, I can only encourage other academics to push for openness of those contracts at their own institutions.&lt;/p&gt;
&lt;p&gt;(For a cool application of machine learning to the process of teaching, look also at the video around the 4:30 mark. Google seems to focus there on content rather than users, and to extract values from the student contributions rather than their private data. They effectively intend to crowdsource smarter compilers.)&lt;/p&gt;
&lt;p&gt;(I want to thank Ignacio Despujol Zabala for letting me know about this Google I/O session.)&lt;/p&gt;</description><category>coursera</category><category>edtech</category><category>google</category><category>privacy</category><guid>http://paulolivier.dehaye.org/posts/edtech-policies-part-i.html</guid><pubDate>Tue, 09 Sep 2014 10:23:58 GMT</pubDate></item><item><title>How fast the world has changed</title><link>http://paulolivier.dehaye.org/posts/how-fast-the-world-has-changed.html</link><dc:creator>Paul-Olivier Dehaye</dc:creator><description>&lt;p&gt;As I am starting to blog, I am taking a course on &lt;a class="reference external" href="http://schoolofopen.p2pu.org/"&gt;openness with P2PU&lt;/a&gt;. This is helping me think of how to quickly and most efficiently &lt;a class="reference external" href="http://opencontent.org/blog/archives/3393"&gt;reclaim my domain&lt;/a&gt;. For this, I have been looking back at the few notes I have posted around on Facebook. Actually, you can too if you have a Facebook account, as I have just made them &lt;a class="reference external" href="https://www.facebook.com/paulolivier.dehaye/notes"&gt;viewable to every facebook user&lt;/a&gt;. This way, if you are new here, you can get a quick sense of what I am about. Incidentally, due to a little known item in the Facebook &lt;a class="reference external" href="https://www.facebook.com/about/privacy/your-info-on-fb"&gt;privacy policy&lt;/a&gt;, this means that all the comments are now public as well (i.e. my friends gave the rights to their comments to Facebook, I gave the right to my original posts to Facebook, and finally Facebook decides to correlate privacy of comments to the privacy settings of the original post). I suppose every platform owner has to make lots of those decisions, which eventually shape the service.&lt;/p&gt;
&lt;p&gt;In any case, one of the notes struck me. It was just a link, actually, to a 2008 &lt;a class="reference external" href="http://warner.blogs.nytimes.com/2008/12/04/first-the-bad-news/"&gt;New York Times story&lt;/a&gt; about an unfortunate Walmart employee who was trampled to death on Black Friday. The angle is that bad things happen in the world, and sometimes this can end up on the internet, &lt;a class="reference external" href="https://www.youtube.com/watch?v=7aUwmsi6Wc0"&gt;filmed on crappy cell phone cameras&lt;/a&gt;. And then we have to tell kids about all that violence. The undertone of the piece is that we were just starting to grapple with that problem back then. A friend of mine commented and asked for my opinion. The original note is &lt;a class="reference external" href="https://www.facebook.com/notes/paul-olivier-dehaye/kids-are-scary-sometimes/103572210077"&gt;here&lt;/a&gt;, with a contemporary screenshot below.&lt;/p&gt;
&lt;img alt="../walmart.jpg" class="align-center" src="http://paulolivier.dehaye.org/walmart.jpg" style="width: 629.0px; height: 307.0px;"&gt;&lt;p&gt;Close to six years later, we are still facing the same problems, of course. Internet is more &lt;a class="reference external" href="http://www.mercurynews.com/business/ci_26459616/youtube-twitter-teamed-halt-spread-beheading-video-report"&gt;violent&lt;/a&gt; and more &lt;a class="reference external" href="http://www.forbes.com/sites/emmawoollacott/2014/09/08/reddit-gives-mixed-messages-after-pulling-leaked-celebrity-photos/"&gt;invasive&lt;/a&gt; than ever. Any platform owner knows the value of good filters to curate content for its users. This content curation can be done jointly by machines and humans, leading to risks of algorithmic bias still misunderstood (&lt;a class="reference external" href="http://www.forbes.com/sites/dailymuse/2014/08/04/the-facebook-experiment-what-it-means-for-you/"&gt;Facebook&lt;/a&gt;, &lt;a class="reference external" href="https://medium.com/message/the-algorithm-giveth-but-it-also-taketh-b7efad92bc1f"&gt;Twitter&lt;/a&gt;). It can also be done exclusively by humans, operating under strict rules. For both posts and comments. This is the model applied by &lt;a class="reference external" href="http://www.metafilter.com"&gt;metafilter&lt;/a&gt;, leading to high quality output but a relatively weak business model, unfortunately &lt;a class="reference external" href="http://gigaom.com/2014/05/22/if-a-high-quality-site-like-metafilter-can-be-crushed-by-google-what-hope-do-other-sites-have/"&gt;still vulnerable to algorithmic whims&lt;/a&gt;.&lt;/p&gt;
&lt;blockquote class="epigraph"&gt;
&lt;dl class="docutils"&gt;&lt;dt&gt;So you're saying that people tried to use the economies of scale of the internet to disrupt the conventional and somewhat hidebound traditional methods and then it turns out that certain things requiring human eyeballs and judgment do not actually scale along with this stuff and the lesson is that you need to keep people in the mix in not just token ways even if this interferes with your bottom line...? I know that song!&lt;/dt&gt;
&lt;dd&gt;-- Metafilter user and former moderator &lt;a class="reference external" href="http://www.metafilter.com/user/292"&gt;jessamyn&lt;/a&gt;&lt;/dd&gt;
&lt;/dl&gt;&lt;/blockquote&gt;
&lt;p&gt;In any case, eight years after this Facebook note, the world still turns around. People get married, have babies, raise their children. And most people still use Facebook.&lt;/p&gt;</description><category>baby</category><category>facebook</category><category>metafilter</category><category>privacy</category><category>reclaim</category><category>violence</category><category>whyopen</category><guid>http://paulolivier.dehaye.org/posts/how-fast-the-world-has-changed.html</guid><pubDate>Mon, 08 Sep 2014 13:37:35 GMT</pubDate></item><item><title>"Don't be evil", or how I learned to behave like a startup and love the data</title><link>http://paulolivier.dehaye.org/posts/dont-be-evil-or-how-i-learned-to-behave-like-a-startup-and-love-the-data.html</link><dc:creator>Paul-Olivier Dehaye</dc:creator><description>&lt;img alt="../strangelove.png" class="align-right" src="http://paulolivier.dehaye.org/strangelove.png" style="width: 320.0px; height: 240.0px;"&gt;&lt;p&gt;When Gmail was opened in 2004, I received invitations early. If I remember well, they came from a friend working at Google who had already snatched a few fun login names. I did the same, and passed on further invitations to my brother and our friends back home.&lt;/p&gt;
&lt;p&gt;A year or so later, when my brother was visiting with his friends, we went on a tour of the Googleplex. Randomly passing in front of the cubicle of a homonym, one of the friends suddenly realised why he had not been able to register his own name earlier. In other words, &lt;strong&gt;an unknown collision in the physical world had first manifested digitally&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;I like to think of those collisions as the digital equivalent of New York overcrowding, trying to fit too many people in just a few login characters.&lt;/p&gt;
&lt;p&gt;So which fun pseudonyms did we chose? Which did we consider worthy in this land grab? Certainly many of them were aimed at our shared cultural backgrounds as Belgians in the Silicon Valley. If you had &lt;a class="reference external" href="mailto:tintin@gmail.com"&gt;tintin@gmail.com&lt;/a&gt;, or &lt;a class="reference external" href="http://nl.wikipedia.org/wiki/Frietkot"&gt;frietkot@gmail.com&lt;/a&gt; that would be pretty impressive, no? Indeed, we grabbed names of regions, superheroes, movie stars, concepts, etc. We certainly thought this was OK, and didn't reflect more on something that &lt;a class="reference external" href="http://en.wikipedia.org/wiki/Nymwars"&gt;became controversial only later&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;One of the logins I grabbed had the name of a Belgian politician, let's call him Some Guy. He was on TV and I thought my friends would get a chuckle if I emailed them from it. Certainly, I might have crossed a moral line already then, but it felt like a very tiny escalation in this virtual land grab.&lt;/p&gt;
&lt;p&gt;What did I do with this account? I mostly used it for spam protection. I set it up so that all emails sent there would be forwarded to my default inbox, and gave this address whenever there was a need to a register for a spammy online service. This worked well, possibly because Gmail's algorithms had learned to weigh emails transiting through this address differently and benefited from the additional segmenting.&lt;/p&gt;
&lt;p&gt;Around 2008, inevitably, I started receiving emails addressed to That Guy. Those collisions happen to all of us, for all of our email accounts. What is the moral thing to do there? My philosophy is most of the time to let it drop, but  sometimes also to reply to the sender telling them that they got the wrong address (due to emails missent to my main account, I must have had to contact a dozen hotels in Quebec by now). In most cases, the only way to know what to do is to read the email, slightly invading this other persons' privacy.&lt;/p&gt;
&lt;blockquote&gt;
Just like Rachel and her Friends in their New York apartment, we struggle to deal with those privacy collisions, especially when we feel a need to intervene.&lt;/blockquote&gt;
&lt;iframe width="900" height="600" src="//www.youtube.com/embed/tYn8s0_kDUw?rel=0&amp;amp;hd=1&amp;amp;wmode=transparent"&gt;&lt;/iframe&gt;&lt;p&gt;For That Guy, it was even easier to feel morally OK about it: I never actively sought the emails, had no way to prevent the mistake, and anyways the emails were from cranks. On top, by that time I had registered to too many services with that pseudonym, which effectively tied my identity to it, with no way to revert the situation. So in effect this data collection was happening, whether I liked it or not, or at least that was my moral justification.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The problem with data is that it leaks.&lt;/strong&gt; The cranks don't just email one influential person at a time. They email a few, who are susceptible to know each other. As a consequence, in this case, the cranks polluted those recipients' email software  with a wrong email address. Of course, in due time, the email autocompletion software of those recipients started tripping them and I started receiving emails from other politicians to That Guy. &lt;strong&gt;Algorithmic curation had gone wrong, and actively mislead humans.&lt;/strong&gt; The fact that these were politicians might have mislead me: I should have made the effort of explaining the awkward situation to That Guy's interlocutors and tried to correct it. But I didn't. Somehow a couple more emails made it to me that were clearly of more social nature. Again, I didn't do anything. &lt;strong&gt;This data will not disappear unless actively deleted&lt;/strong&gt;, and even then I can only be so sure.&lt;/p&gt;
&lt;blockquote&gt;
At this point you will deservedly think that I am a moron. But was it morally wrong? And when exactly did it go wrong?&lt;/blockquote&gt;
&lt;p&gt;Throughout my moral justification was that I was not actively seeking this. Emails would land in my mailbox and I would have to read them to know what to do. Of course, this conveniently ignores what I could have done to prevent those emails to arrive in the first place. Part of my justification was that I wasn't doing anything with the data collected. There was no clear goal, except &lt;strong&gt;awareness that this could be used to make a point later, which I guess I am making here now publicly&lt;/strong&gt; (in fact, I have used this to make this point in private throughout the years).&lt;/p&gt;
&lt;p&gt;The more interesting issue here is to understand that this is exactly how many big data companies function. "Don't be evil" Google gobbles data all over the place &lt;a class="reference external" href="http://www.wired.com/2014/04/threatlevel_0401_streetview/"&gt;for purposes that are not always clear at the time&lt;/a&gt;, and the justification is often that this was incidental, automated and did not require human intervention. Looking at a corporate setting elevates the stakes, and my feeble moral justifications are not sufficient anymore. It becomes a matter of ethics, which arguably should be that data collection is by default unethical: data should not be kept beyond the time necessary for its intended use, with that use itself subject to precise and established ethical rules. It looks like Google has understood this in some markets, for instance education (unlike other players there), and this will be the topic of a later post.&lt;/p&gt;
&lt;p&gt;(Image in the public domain: the Dr Stangelove War Room, which &lt;a class="reference external" href="http://valleywag.gawker.com/airbnbs-office-has-a-replica-of-the-dr-strangelove-wa-1475788543"&gt;happens to be replicated in the Airbnb HQ&lt;/a&gt;)&lt;/p&gt;</description><category>ethics</category><category>privacy</category><guid>http://paulolivier.dehaye.org/posts/dont-be-evil-or-how-i-learned-to-behave-like-a-startup-and-love-the-data.html</guid><pubDate>Mon, 08 Sep 2014 09:45:51 GMT</pubDate></item><item><title>(Social) teaching machines</title><link>http://paulolivier.dehaye.org/posts/social-teaching-machines.html</link><dc:creator>Paul-Olivier Dehaye</dc:creator><description>&lt;p&gt;I want to take the opportunity of the recent talk by Audrey Watters on &lt;a class="reference external" href="http://hackeducation.com/2014/09/03/monsters-altc2014/"&gt;Ed-Tech's Monsters&lt;/a&gt; to share some of my thoughts.&lt;/p&gt;
&lt;p&gt;In her talk, Watters physically bases herself at Bletchley Park, a place of invention, ingenuity and deceit that greatly contributed to the Allies' war effort and incidentally the evolution of computing. She then steps back in time to Ludd and his followers, who rebelled against the introduction of machinery in their work. She then segways into the Frankenstein story of a creation abandoned by its master and finally draws parallels with the situation in ed tech today and the "promises" of teaching machines. This was an impossibly bad and short summary of a very good talk, so I would highly recommend to any reader lost here to go read the original. After doing that, please come back.&lt;/p&gt;
&lt;iframe width="900" height="600" src="//www.youtube.com/embed/6qwZm56UadE?rel=0&amp;amp;hd=1&amp;amp;wmode=transparent"&gt;&lt;/iframe&gt;&lt;p&gt;During World War II, cryptographers worked at Bletchley Park to decipher German and Japanese secret messages. These were encoded by various versions of a machine called &lt;a class="reference external" href="http://en.wikipedia.org/wiki/Enigma_machine"&gt;Enigma&lt;/a&gt;, a typewriter wired with multiple electrical contacts that constantly shuffled letters around. It is really a quite dumb but very messy and obfuscated process, with one useful property: at any stage it created an &lt;a class="reference external" href="http://en.wikipedia.org/wiki/Involution_(mathematics)"&gt;involution&lt;/a&gt;, so one can use the same machine to both encrypt and decrypt. You can see little Millie demonstrating this in the video above (shot in September 2013 by yours truly).&lt;/p&gt;
&lt;img alt="../bombe-front.jpg" class="align-right" src="http://paulolivier.dehaye.org/bombe-front.jpg" style="width: 365.04px; height: 273.6px;"&gt;&lt;p&gt;To attack the ciphered messages, the cryptologists at Bletchley Park did not build full on computers, but instead  machines that could simulate many Enigmas in parallel (36 Enigmas per &lt;a class="reference external" href="http://en.wikipedia.org/wiki/Bombe"&gt;Turing Bombe&lt;/a&gt;). These also had extra wiring which encoded additional properties of the Enigma protocol. On any given day, around 200 of these machines were used to recover the common settings for all the encrypted messages sent that day.&lt;/p&gt;
&lt;p&gt;What has always fascinated me with Blecthley Park is the &lt;strong&gt;subtle interplay between humans and the machines&lt;/strong&gt;. While the heavy computations were done by those Bombes, the British did not seek/manage to automate everything. Some steps were always left to manual labor, most notably message passing and picking the initial input. The initial input was known as a &lt;a class="reference external" href="http://en.wikipedia.org/wiki/Cryptanalysis_of_the_Enigma#Crib-based_decryption"&gt;crib&lt;/a&gt;, and  is essentially an informed guess at partial plain text. It seems to have always been more of an art than a science to obtain, even involving some psychology to know where to look. By message passing, I mean that Bletchley Park was not just a bunch of machines: these were disconnected, so there had to actually be many people transcribing output from one machine, making relatively simple decisions (all lights lit!) and entering that output into the next machine. There were several good reasons to do it this way. It is easier to train staff than build a new and complex sorting machine. On top, war is messy, and the Germans changed their procedures several times, requiring agility in the workflow (the Germans were less likely to change their hardware). Over the course of the war, there was constant prototyping of different workflows around core mechanical infrastructure, and this experience helped abstract the generic modern computer (formally, a &lt;a class="reference external" href="http://en.wikipedia.org/wiki/Turing_machine"&gt;Turing Machine&lt;/a&gt;) and eventually build it (unlike Charles Babbage's machine and Ada Lovelace's programs which remained theoretical).&lt;/p&gt;
&lt;blockquote&gt;
The lesson of Bletchley Park is that it is sometimes easier but sufficient to build a social machine rather than a fully automated machine.&lt;/blockquote&gt;
&lt;p&gt;A &lt;a class="reference external" href="http://en.wikipedia.org/wiki/Social_machine"&gt;social machine&lt;/a&gt; is an environment comprising humans and technology interacting and producing outputs or action which would not be possible without both parties present. The term became popular thanks to Tim Berners-Lee, who anticipated them along with the World Wide Web.&lt;/p&gt;
&lt;img alt="../bombe-back.jpg" class="align-left" src="http://paulolivier.dehaye.org/bombe-back.jpg" style="width: 365.04px; height: 273.6px;"&gt;&lt;p&gt;Facebook, for instance, serves as a social machine in multiple ways. You can pass on content to your friends, which they can "like" and comment on, they can tag you in pictures, etc. Every such action is logged, and this machine has a unique goal: to know you better and serve you more valuable ads. All the necessary information has to be volunteered by humans because machines would not have been able to guess it on their own. Sometimes this social machine actively uses your friends to disclose information you might have wanted to keep private, &lt;a class="reference external" href="http://www.digitaltrends.com/social-media/what-exactly-is-a-facebook-shadow-profile/#!bP6C2L"&gt;even if you are not a Facebook user!&lt;/a&gt; Kids ratting on their parents, in other words.&lt;/p&gt;
&lt;p&gt;How is this relevant to ed tech? One of the most successful social machines in education is &lt;a class="reference external" href="http://duolingo.com"&gt;Duolingo&lt;/a&gt;. While it offers students the option of learning a language, it is really built with the intention of translating the web, and uses a creative setup to find and motivate participants. Even the course creation is now crowdsourced, via its &lt;a class="reference external" href="http://incubator.duolingo.com/"&gt;incubator&lt;/a&gt;. MOOCs actually tend to rely heavily on the same type of crowdsourcing. Course creation is crowdsourced to &lt;strong&gt;professors, who can create custom social machines tailored to the topic at hand&lt;/strong&gt;. When the course is run, this machine collects information about who is a good student, who is bad, who is a deep thinker, who is meticulous, who defines their own path, etc. Eventually, the goal might be to evaluate all these characteristics at scale algorithmically (despite all the risks for algorithmic bias that this entails), but the key point is that it can be "faked" at first: via peer feedback and rubric grading, one can use power relationships to inject fairly complicated judgements into this machine, at scale, with little cost.&lt;/p&gt;
&lt;p&gt;Similarly, other relatively complex MOOC services are also sometimes crowdsourced, such as translating course materials, or mutual technical support for the professors and students. One can expect that some of these tasks will eventually also be automated: for instance, some MOOC platforms already use intelligent agents (robots masquerading as humans) to answer student questions in the forums.&lt;/p&gt;
&lt;blockquote&gt;
MOOC platforms offer the option to professors to easily stand up their own social machines. What should be their purpose? Who should be responsible for them?&lt;/blockquote&gt;
&lt;p&gt;Watters insist that Luddites were not rejecting technology, but rather rejecting exploitation. Crowdsourcing already carries significant risks of exploitation, particularly in the &lt;a class="reference external" href="http://florianschmidt.co/the-good-the-bad-and-the-ugly/"&gt;domain of intellectual property&lt;/a&gt;, but this is not the only one. In another talk, she says that &lt;a class="reference external" href="http://hackeducation.com/2013/10/17/student-data-is-the-new-oil/"&gt;"Student data is the new oil"&lt;/a&gt;. Indeed, this seems to be another path that all the big MOOC providers have chosen so far. A professor building a MOOC only helps the platform collect more private information about its users, maybe even under the guise of improving their user experience. But for what purpose exactly? Which engine is running off this oil? Where is it headed? Is this data helping for research in education? In social science? In human-computer interaction? Or simply for profit, selling that data to the highest bidder/best revenue model, without moral guidance? All these options are actively pursued right now, sometimes simultaneously, and professors preparing a MOOC should give great pause to these issues and think carefully at the setting where they have decided to do so. Possibly they might have to fight for the luxury of picking this setting. Professors have a lot of moral responsibility towards the students (the weakest cogs by far in this social machine), to make sure that the free-education-for-all mantra does not turn into another form of exploitation. Do these professors even fully understand the situation? Do they fully understand how &lt;a class="reference external" href="https://www.youtube.com/watch?v=ldhHkVjLe7A"&gt;free-is-a-lie&lt;/a&gt;? Who carries the responsibility of informing them?&lt;/p&gt;
&lt;blockquote class="epigraph"&gt;
&lt;p&gt;Research conducted without applied ethics is morally bankrupt because when scientists lack morals, outside sources can more easily manipulate their work for destructive purposes. In such situations, scientists are likely to adopt the rationalizations of that party to justify their efforts.&lt;/p&gt;
&lt;p class="attribution"&gt;—&lt;a class="reference external" href="http://ashbrook.org/publications/respub-v8n1-cook/"&gt;Erica Cook&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In the Bletchley Park analogy, this moral responsibility is eclipsed by the dramatic circumstances of war. One person encodes, the other one decodes, some people die, some survive but at least it feels fair (to me at least, maybe blinded by my mathematicians' background). Yet the parallels outlined above remain as strong with Los Alamos and the atomic bomb, where Feynman was reorganising his own social machine to &lt;a class="reference external" href="http://youtu.be/0ogSC6JKkrY?t=47m50s"&gt;perform simulations of atomic explosions&lt;/a&gt;, even holding competitions pitting his chimeric machine against actual IBM computers. Certainly, the ethical questions are more nagging with Los Alamos, but on either side of the Atlantic the machine operators never had the opportunity to raise concerns with what they were contributing to. In an environment full of (male) generals and (male) scientists the machines were mostly &lt;a class="reference external" href="http://www.mkheritage.co.uk/bpt/Women/wrensOS.html"&gt;"manned" by women&lt;/a&gt;, within a society that didn't even pretend to give them an equal voice. After the war, many ethical question &lt;a class="reference external" href="http://ashbrook.org/publications/respub-v8n1-cook/"&gt;hung squarely  and solely on the scientists' shoulders&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;So if we don't like the current MOOC models, what should be the way forward?&lt;/p&gt;
&lt;blockquote&gt;
Luddites were seeking to disrupt the technological disruption, and we as professors should seek to do the same.&lt;/blockquote&gt;
&lt;p&gt;In fact, one might argue this is part of our job, to help society move forward without fear of a challenge, criticism or controversy, as long as we can back our arguments with evidence. In today's conversation, business logic has misappropriated the words &lt;a class="reference external" href="http://en.wikipedia.org/wiki/Clayton_M._Christensen"&gt;"disruption" and "innovation"&lt;/a&gt; and mostly tied them with technology. In fact, disruption is to be found anywhere in academia, if not more in the humanities: "why?" is a more powerful question than "how?". Certainly some MOOC providers have opened themselves up to this disruption themselves, through &lt;a class="reference external" href="https://www.youtube.com/watch?v=ODL-owGjti8"&gt;overinflated claims of fixing with old technology something that was not necessarily broken&lt;/a&gt;. MOOCs are a fantastic opportunity to build truly new ways of learning, collaborating, discovering and generally helping society progress through exchange of information.&lt;/p&gt;
&lt;p&gt;Getting these improved MOOCs off the ground will be hard. It will require dedication, transparency, freedom to tinker, accountability and tolerance for failure. Above all, it will require rock solid ethical ground, which is too easy to compromise in a competitive environment mixing academia and its "strategic relationships".&lt;/p&gt;</description><category>connected_course</category><category>crypto</category><category>edtech</category><category>social_machine</category><guid>http://paulolivier.dehaye.org/posts/social-teaching-machines.html</guid><pubDate>Thu, 04 Sep 2014 00:46:03 GMT</pubDate></item><item><title>First day at new job</title><link>http://paulolivier.dehaye.org/posts/first-day-at-new-job.html</link><dc:creator>Paul-Olivier Dehaye</dc:creator><description>&lt;a class="reference external" href="https://github.com/pdehaye/pdehaye.github.io/commit/941d5a38fb52a59af3d9e658d021d0d322ce0034#diff-100d13379c12af3fb41203dc73dc396e"&gt;Got one of each now.&lt;/a&gt; Mom is doing fine. Will do my best. Hope to be up to the task.</description><category>baby</category><guid>http://paulolivier.dehaye.org/posts/first-day-at-new-job.html</guid><pubDate>Wed, 03 Sep 2014 19:26:46 GMT</pubDate></item></channel></rss>